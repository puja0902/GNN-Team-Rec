{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puja0902/App/blob/master/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eUf_9G8c8RNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3-files for keys(authors list), values(fos list) and ratings\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path for the input file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the paths for the output files\n",
        "output_keys_file = '/content/drive/MyDrive/keys.txt'\n",
        "output_values_file = '/content/drive/MyDrive/values.txt'\n",
        "output_ratings_file = '/content/drive/MyDrive/ratings.txt'\n",
        "\n",
        "# Write keys to the output file with a comma at the end of each line\n",
        "with open(output_keys_file, 'w') as keys_file:\n",
        "    keys_file.write(','.join(keys).replace(',', ',\\n'))\n",
        "\n",
        "# Iterate through values and ratings\n",
        "for i in range(max(len(v) for v in data.values())):\n",
        "    values = []\n",
        "    ratings = []\n",
        "\n",
        "    for key, value in data.items():\n",
        "        if i < len(value):\n",
        "            values.append(list(value.keys())[i] + ',')\n",
        "            ratings.append(str(list(value.values())[i]) + ',')\n",
        "\n",
        "    # Write values to the output file\n",
        "    with open(output_values_file, 'a') as values_file:\n",
        "        values_file.write('\\n'.join(values) + '\\n')\n",
        "\n",
        "    # Write ratings to the output file\n",
        "    with open(output_ratings_file, 'a') as ratings_file:\n",
        "        ratings_file.write('\\n'.join(ratings) + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1. To make network of co-authors\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store authors and their co-authors\n",
        "author_to_coauthors = {}\n",
        "\n",
        "# Function to add co-authors for an author\n",
        "def add_coauthors(authors):\n",
        "    for author in authors:\n",
        "        if author not in author_to_coauthors:\n",
        "            author_to_coauthors[author] = set()  # Use a set to ensure uniqueness\n",
        "        author_to_coauthors[author].update(authors)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/dblp_papers_v11.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract authors for the current paper\n",
        "        authors = [author_info['name'] for author_info in data.get('authors', [])]\n",
        "\n",
        "        # Add co-authors for the current paper\n",
        "        add_coauthors(authors)\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Convert sets to lists for JSON serialization\n",
        "author_to_coauthors = {author: list(coauthors) for author, coauthors in author_to_coauthors.items()}\n",
        "\n",
        "# Save the author-to-coauthors dictionary to a JSON file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_to_coauthors, output_file, indent=2)\n",
        "\n",
        "print(f\"Author to Co-authors (with updates) saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# 2. fos-to-authors and authors-to-fos code\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store authors and their co-authors\n",
        "author_to_coauthors = {}\n",
        "\n",
        "# Function to add co-authors for an author\n",
        "def add_coauthors(authors):\n",
        "    for author in authors:\n",
        "        if author not in author_to_coauthors:\n",
        "            author_to_coauthors[author] = set()  # Use a set to ensure uniqueness\n",
        "        author_to_coauthors[author].update(authors)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/dblp_papers_v11.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract authors for the current paper\n",
        "        authors = [author_info['name'] for author_info in data.get('authors', [])]\n",
        "\n",
        "        # Add co-authors for the current paper\n",
        "        add_coauthors(authors)\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Convert sets to lists for JSON serialization\n",
        "author_to_coauthors = {author: list(coauthors) for author, coauthors in author_to_coauthors.items()}\n",
        "\n",
        "# Save the author-to-coauthors dictionary to a JSON file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_to_coauthors, output_file, indent=2)\n",
        "\n",
        "print(f\"Author to Co-authors (with updates) saved to {output_file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# 3. authors their fos with weights\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store author FOS with aggregated weights\n",
        "author_fos_dict = {}\n",
        "\n",
        "# Specify the input file path\n",
        "input_file_path = '/content/drive/MyDrive/dblp_papers_v11.txt'  # Replace with your file path\n",
        "\n",
        "# Read and process each line in the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Load each line as a JSON object\n",
        "        paper = json.loads(line)\n",
        "\n",
        "        authors = paper.get('authors', [])\n",
        "        fos_list = paper.get('fos', [])\n",
        "\n",
        "        # Iterate through authors for each paper\n",
        "        for author in authors:\n",
        "            author_name = author['name']\n",
        "\n",
        "            if author_name not in author_fos_dict:\n",
        "                author_fos_dict[author_name] = {}\n",
        "\n",
        "            # Create a dictionary to store aggregated weights for each FOS for this author\n",
        "            aggregated_weights = {}\n",
        "\n",
        "            # Iterate through FOS for the paper and aggregate weights\n",
        "            for fos in fos_list:\n",
        "                fos_name = fos['name']\n",
        "                fos_weight = fos['w']\n",
        "\n",
        "                # Aggregate weights for the same FOS for this author\n",
        "                if fos_name in aggregated_weights:\n",
        "                    aggregated_weights[fos_name].append(fos_weight)\n",
        "                else:\n",
        "                    aggregated_weights[fos_name] = [fos_weight]\n",
        "\n",
        "            # Calculate the average weights for each FOS for this author\n",
        "            for fos, weights in aggregated_weights.items():\n",
        "                average_weight = sum(weights) / len(weights)\n",
        "                author_fos_dict[author_name][fos] = average_weight\n",
        "\n",
        "# Save the author FOS data with aggregated weights to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/weights.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_fos_dict, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# 4. authors their fos with opinions\n",
        "\n",
        "import json\n",
        "\n",
        "# Define the opinion embeddings\n",
        "opinion_embeddings = {\n",
        "    \"0\": (0.0, 0.125),\n",
        "    \"1\": (0.125, 0.25),\n",
        "    \"2\": (0.25, 0.375),\n",
        "    \"3\": (0.375, 0.5),\n",
        "    \"4\": (0.5, 0.625),\n",
        "    \"5\": (0.625, 0.75),\n",
        "    \"6\": (0.75, 0.875),\n",
        "    \"7\": (0.875, 1.0)\n",
        "}\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Function to assign opinion based on weight\n",
        "def assign_opinion(weight):\n",
        "    for opinion, (start, end) in opinion_embeddings.items():\n",
        "        if start <= weight <= end:\n",
        "            return opinion\n",
        "    return None\n",
        "\n",
        "# Create a dictionary to store author opinions\n",
        "output_data = {}\n",
        "\n",
        "# Iterate through authors and concepts in the input data\n",
        "for author, concepts in input_data.items():\n",
        "    author_opinions = {}\n",
        "    for concept, weight in concepts.items():\n",
        "        opinion = assign_opinion(weight)\n",
        "        if opinion:\n",
        "            author_opinions[concept] = opinion\n",
        "    output_data[author] = author_opinions\n",
        "\n",
        "# Save the output data to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinion.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/opinion.json'\n",
        "\n",
        "# Read input JSON from the file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Create a new dictionary for the desired output\n",
        "output_dict = {}\n",
        "\n",
        "# Iterate through the input data and populate the output dictionary\n",
        "for author, fos_data in data.items():\n",
        "    for fos, value in fos_data.items():\n",
        "        if fos not in output_dict:\n",
        "            output_dict[fos] = {}\n",
        "        output_dict[fos][author] = value\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinionItem.json'\n",
        "\n",
        "# Write the output JSON to a file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_dict, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#get Authors list\n",
        "\n",
        "import json\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/authors_to_fos.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_keys_file = '/content/drive/MyDrive/getAuthors.txt'\n",
        "\n",
        "# Write keys to the output file with comma separation and new line\n",
        "with open(output_keys_file, 'w') as output_file:\n",
        "    for key in keys:\n",
        "        output_file.write(key + ',\\n')\n",
        "\n",
        "\n",
        "\n",
        "#get fos list\n",
        "\n",
        "import json\n",
        "\n",
        "input_file_path = '/content/drive/MyDrive/fos_to_authors.json'\n",
        "\n",
        "# Read data from the input file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract keys from the data\n",
        "keys = list(data.keys())\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_keys_file = '/content/drive/MyDrive/getFos.txt'\n",
        "\n",
        "# Write keys to the output file with comma separation and new line\n",
        "with open(output_keys_file, 'w') as output_file:\n",
        "    for key in keys:\n",
        "        output_file.write(key + ',\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Split the key (Author) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/keys.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "# Split the values (Fos) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/values.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainFos.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testFos.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "# Split the values (Rating) data into training and testing\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/ratings.txt'\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/trainRating.txt'\n",
        "output_test_file_path = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets\n",
        "train_lines = lines[:split_index]\n",
        "test_lines = lines[split_index:]\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    train_file.writelines(train_lines)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    test_file.writelines(test_lines)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the file paths for the JSON and TXT files\n",
        "json_file_paths = ['/content/drive/MyDrive/smallauthors_to_fos.json', '/content/drive/MyDrive/smallopinion.json',\n",
        "                   '/content/drive/MyDrive/smallfos_to_authors.json', '/content/drive/MyDrive/smallopinionItem.json']\n",
        "# txt_file_paths = ['/content/drive/MyDrive/trainAuthor.txt', '/content/drive/MyDrive/trainFos.txt', '/content/drive/MyDrive/trainRating.txt',\n",
        "#                   '/content/drive/MyDrive/testAuthor.txt', '/content/drive/MyDrive/testFos.txt', '/content/drive/MyDrive/testRating.txt']\n",
        "\n",
        "\n",
        "# Specify the output file path for the combined data\n",
        "output_file_path = '/content/drive/MyDrive/smallcombinedFile.json'\n",
        "\n",
        "# Initialize an empty list to store individual file data\n",
        "all_data = []\n",
        "\n",
        "# Read data from JSON files\n",
        "for json_path in json_file_paths:\n",
        "    with open(json_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    all_data.append(data)\n",
        "\n",
        "# Read data from TXT files\n",
        "# for txt_path in txt_file_paths:\n",
        "#     with open(txt_path, 'r') as txt_file:\n",
        "#         data = {'text_data': txt_file.read()}\n",
        "#     all_data.append(data)\n",
        "\n",
        "\n",
        "\n",
        "# Write the combined data to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(all_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Specify the file paths for the text files\n",
        "file1_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "file2_path = '/content/drive/MyDrive/trainFos.txt'\n",
        "file3_path = '/content/drive/MyDrive/trainRating.txt'\n",
        "file4_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "file5_path = '/content/drive/MyDrive/testFos.txt'\n",
        "file6_path = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "# Read data from the first file and split it into a list\n",
        "with open(file1_path, 'r') as file1:\n",
        "    data1 = [word.strip() for word in file1.read().split(',')]\n",
        "\n",
        "# Read data from the second file and split it into a list\n",
        "with open(file2_path, 'r') as file2:\n",
        "    data2 = [word.strip() for word in file2.read().split(',')]\n",
        "\n",
        "# Read data from the third file and split it into a list\n",
        "with open(file3_path, 'r') as file3:\n",
        "    data3 = [word.strip() for word in file3.read().split(',')]\n",
        "\n",
        "\n",
        "# Read data from the first file and split it into a list\n",
        "with open(file4_path, 'r') as file4:\n",
        "    data4 = [word.strip() for word in file4.read().split(',')]\n",
        "\n",
        "# Read data from the second file and split it into a list\n",
        "with open(file5_path, 'r') as file5:\n",
        "    data5 = [word.strip() for word in file5.read().split(',')]\n",
        "\n",
        "# Read data from the third file and split it into a list\n",
        "with open(file6_path, 'r') as file6:\n",
        "    data6 = [word.strip() for word in file6.read().split(',')]\n",
        "\n",
        "\n",
        "# Combine the three lists into a single list\n",
        "combined_data = [data1, data2, data3, data4, data5, data6]\n",
        "\n",
        "# Write the combined data to the output file\n",
        "output_file_path = '/content/drive/MyDrive/combinedFile1.txt'\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for item in combined_data:\n",
        "        output_file.write('[' + ',\\n'.join(item) + '],\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Specify the path for the input file\n",
        "input_file_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Read the content from the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Replace [ with { and ] with }\n",
        "modified_content = content.replace('[', '\"{').replace(']', '}\"')\n",
        "\n",
        "# Specify the path for the output file\n",
        "output_file_path = '/content/drive/MyDrive/author_network.json'\n",
        "\n",
        "# Write the modified content back to the output file\n",
        "with open(output_file_path, 'w') as file:\n",
        "    file.write(modified_content)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# convert json to pickle\n",
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/smalltestAuthor.json'\n",
        "input_file_path1 = '/content/drive/MyDrive/smalltestFos.json'\n",
        "input_file_path2 = '/content/drive/MyDrive/smalltestRating.json'\n",
        "input_file_path3 = '/content/drive/MyDrive/smalltrainAuthor.json'\n",
        "input_file_path4 = '/content/drive/MyDrive/smalltrainFos.json'\n",
        "input_file_path5 = '/content/drive/MyDrive/smalltrainRating.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/smalltestAuthor.pkl'\n",
        "pickle_file_path1 = '/content/drive/MyDrive/smalltestFos.pkl'\n",
        "pickle_file_path2 = '/content/drive/MyDrive/smalltestRating.pkl'\n",
        "pickle_file_path3 = '/content/drive/MyDrive/smalltrainAuthor.pkl'\n",
        "pickle_file_path4 = '/content/drive/MyDrive/smalltrainFos.pkl'\n",
        "pickle_file_path5 = '/content/drive/MyDrive/smalltrainRating.pkl'\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(input_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path1, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path2, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path3, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path4, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path5, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path1, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path2, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path3, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path4, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path5, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "# convert text to pickle\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "txt_file_path = '/content/drive/MyDrive/combinedFile-edited.txt'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/combinedFile-edited.pkl'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(txt_file_path, 'r') as txt_file:\n",
        "    data = [line.strip() for line in txt_file.readlines()]\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "# convert text to pickle\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "txt_file_path1 = '/content/drive/MyDrive/author_to_coauthors_opinions.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path1 = '/content/drive/MyDrive/author_to_coauthors_opinions.pkl'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(txt_file_path1, 'r') as txt_file:\n",
        "    data = [line.strip() for line in txt_file.readlines()]\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path1, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "py_file_location = \"/content/drive/MyDrive/GraphRec-WWW19\"\n",
        "sys.path.append(os.path.abspath(py_file_location))\n",
        "\n",
        "\n",
        "# covert txt to json\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_txt_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "input_txt_path1 = '/content/drive/MyDrive/trainFos.txt'\n",
        "input_txt_path2 = '/content/drive/MyDrive/trainRating.txt'\n",
        "input_txt_path3 = '/content/drive/MyDrive/testAuthor.txt'\n",
        "input_txt_path4 = '/content/drive/MyDrive/testFos.txt'\n",
        "input_txt_path5 = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_json_path = '/content/drive/MyDrive/trainAuthor.json'\n",
        "output_json_path1 = '/content/drive/MyDrive/trainFos.json'\n",
        "output_json_path2 = '/content/drive/MyDrive/trainRating.json'\n",
        "output_json_path3 = '/content/drive/MyDrive/testAuthor.json'\n",
        "output_json_path4 = '/content/drive/MyDrive/testFos.json'\n",
        "output_json_path5 = '/content/drive/MyDrive/testRating.json'\n",
        "\n",
        "# Read data from the text file\n",
        "with open(input_txt_path, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path1, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path2, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path3, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path4, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "with open(input_txt_path5, 'r') as txt_file:\n",
        "    # Split lines and create a list\n",
        "    data_list = [line.strip() for line in txt_file]\n",
        "\n",
        "# Write the list to a JSON file\n",
        "with open(output_json_path, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path1, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path2, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path3, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path4, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "with open(output_json_path5, 'w') as json_file:\n",
        "    json.dump(data_list, json_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# Specify the paths for the input text files\n",
        "text_file_paths = [\n",
        "    '/content/drive/MyDrive/testAuthor.txt',\n",
        "    '/content/drive/MyDrive/testFos.txt',\n",
        "    '/content/drive/MyDrive/testRating.txt'\n",
        "]\n",
        "\n",
        "# Specify the path for the output merged text file\n",
        "output_text_path = '/content/drive/MyDrive/testCombined.txt'\n",
        "\n",
        "# Initialize an empty list to store the merged content\n",
        "merged_content = []\n",
        "\n",
        "# Read data from each text file and merge it\n",
        "for text_path in text_file_paths:\n",
        "    with open(text_path, 'r') as text_file:\n",
        "        content = text_file.read()\n",
        "        merged_content.append(content)\n",
        "\n",
        "# Write the merged content to the output text file\n",
        "with open(output_text_path, 'w') as output_file:\n",
        "    for content in merged_content:\n",
        "        output_file.write(content)\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "text_file_path = '/content/drive/MyDrive/testFos.txt'\n",
        "text_file_path1 = '/content/drive/MyDrive/testRating.txt'\n",
        "text_file_path2 = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "text_file_path3 = '/content/drive/MyDrive/trainFos.txt'\n",
        "text_file_path4 = '/content/drive/MyDrive/trainRating.txt'\n",
        "\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/testFos.pkl'\n",
        "output_pickle_path1 = '/content/drive/MyDrive/testRating.pkl'\n",
        "output_pickle_path2 = '/content/drive/MyDrive/trainAuthor.pkl'\n",
        "output_pickle_path3 = '/content/drive/MyDrive/trainFos.pkl'\n",
        "output_pickle_path4 = '/content/drive/MyDrive/trainRating.pkl'\n",
        "\n",
        "\n",
        "# Read data from the text file\n",
        "with open(text_file_path, 'r') as text_file:\n",
        "    text_data = text_file.read()\n",
        "\n",
        "with open(text_file_path1, 'r') as text_file:\n",
        "    text_data1 = text_file.read()\n",
        "\n",
        "with open(text_file_path2, 'r') as text_file:\n",
        "    text_data2 = text_file.read()\n",
        "\n",
        "with open(text_file_path3, 'r') as text_file:\n",
        "    text_data3 = text_file.read()\n",
        "\n",
        "with open(text_file_path4, 'r') as text_file:\n",
        "    text_data4 = text_file.read()\n",
        "\n",
        "# Convert the text data to a pickle object and write to the output file\n",
        "with open(output_pickle_path, 'wb') as output_pickle_file:\n",
        "    pickle.dump(text_data, output_pickle_file)\n",
        "\n",
        "with open(output_pickle_path1, 'wb') as output_pickle_file1:\n",
        "    pickle.dump(text_data1, output_pickle_file1)\n",
        "\n",
        "with open(output_pickle_path2, 'wb') as output_pickle_file2:\n",
        "    pickle.dump(text_data2, output_pickle_file2)\n",
        "\n",
        "with open(output_pickle_path3, 'wb') as output_pickle_file3:\n",
        "    pickle.dump(text_data3, output_pickle_file3)\n",
        "\n",
        "with open(output_pickle_path4, 'wb') as output_pickle_file4:\n",
        "    pickle.dump(text_data4, output_pickle_file4)\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "json_file_path = '/content/drive/MyDrive/smallauthor_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/smallauthor_to_coauthors.pkl'\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(json_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Write the data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input text file\n",
        "text_file_path = '/content/drive/MyDrive/opinions.txt'\n",
        "\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/opinions.pkl'\n",
        "\n",
        "\n",
        "\n",
        "# Read data from the text file\n",
        "with open(text_file_path, 'r') as text_file:\n",
        "    text_data = text_file.read()\n",
        "\n",
        "\n",
        "# Convert the text data to a pickle object and write to the output file\n",
        "with open(output_pickle_path, 'wb') as output_pickle_file:\n",
        "    pickle.dump(text_data, output_pickle_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Generate Smaller- dataset\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "# Load dataset from a pickle file\n",
        "input_pickle_file_path = '/content/drive/MyDrive/testAuthor.pkl'\n",
        "input_pickle_file_path1 = '/content/drive/MyDrive/testFos.pkl'\n",
        "input_pickle_file_path2 = '/content/drive/MyDrive/testRating.pkl'\n",
        "input_pickle_file_path3 = '/content/drive/MyDrive/trainAuthor.pkl'\n",
        "input_pickle_file_path4 = '/content/drive/MyDrive/trainFos.pkl'\n",
        "input_pickle_file_path5 = '/content/drive/MyDrive/trainRating.pkl'\n",
        "input_pickle_file_path6 = '/content/drive/MyDrive/opinions.pkl'\n",
        "input_pickle_file_path7 = '/content/drive/MyDrive/combinedFile.pkl'\n",
        "\n",
        "with open(input_pickle_file_path, 'rb') as pickle_file:\n",
        "    df = pickle.load(pickle_file)\n",
        "\n",
        "with open(input_pickle_file_path1, 'rb') as pickle_file1:\n",
        "    df = pickle.load(pickle_file1)\n",
        "\n",
        "with open(input_pickle_file_path2, 'rb') as pickle_file2:\n",
        "    df = pickle.load(pickle_file2)\n",
        "\n",
        "with open(input_pickle_file_path3, 'rb') as pickle_file3:\n",
        "    df = pickle.load(pickle_file3)\n",
        "\n",
        "with open(input_pickle_file_path4, 'rb') as pickle_file4:\n",
        "    df = pickle.load(pickle_file4)\n",
        "\n",
        "with open(input_pickle_file_path5, 'rb') as pickle_file5:\n",
        "    df = pickle.load(pickle_file5)\n",
        "\n",
        "with open(input_pickle_file_path6, 'rb') as pickle_file6:\n",
        "    df = pickle.load(pickle_file6)\n",
        "\n",
        "with open(input_pickle_file_path7, 'rb') as pickle_file7:\n",
        "    df = pickle.load(pickle_file7)\n",
        "\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "percentage_to_keep = 0.4  # Adjust as needed\n",
        "smaller_df = random.sample(df, int(len(df) * percentage_to_keep))\n",
        "\n",
        "\n",
        "# Save the smaller DataFrame to a new pickle file\n",
        "output_pickle_file_path = '/content/drive/MyDrive/smalltestAuthor.pkl'\n",
        "output_pickle_file_path1 = '/content/drive/MyDrive/smalltestFos.pkl'\n",
        "output_pickle_file_path2 = '/content/drive/MyDrive/smalltestRating.pkl'\n",
        "output_pickle_file_path3 = '/content/drive/MyDrive/smalltrainAuthor.pkl'\n",
        "output_pickle_file_path4 = '/content/drive/MyDrive/smalltrainFos.pkl'\n",
        "output_pickle_file_path5 = '/content/drive/MyDrive/smalltrainRating.pkl'\n",
        "output_pickle_file_path6 = '/content/drive/MyDrive/smallopinions.pkl'\n",
        "output_pickle_file_path7 = '/content/drive/MyDrive/smallcombinedFile.pkl'\n",
        "\n",
        "with open(output_pickle_file_path, 'wb') as output_pickle_file:\n",
        "    pickle.dump(smaller_df, output_pickle_file)\n",
        "\n",
        "with open(output_pickle_file_path1, 'wb') as output_pickle_file1:\n",
        "    pickle.dump(smaller_df, output_pickle_file1)\n",
        "\n",
        "with open(output_pickle_file_path2, 'wb') as output_pickle_file2:\n",
        "    pickle.dump(smaller_df, output_pickle_file2)\n",
        "\n",
        "with open(output_pickle_file_path3, 'wb') as output_pickle_file3:\n",
        "    pickle.dump(smaller_df, output_pickle_file3)\n",
        "\n",
        "with open(output_pickle_file_path4, 'wb') as output_pickle_file4:\n",
        "    pickle.dump(smaller_df, output_pickle_file4)\n",
        "\n",
        "with open(output_pickle_file_path5, 'wb') as output_pickle_file5:\n",
        "    pickle.dump(smaller_df, output_pickle_file5)\n",
        "\n",
        "with open(output_pickle_file_path6, 'wb') as output_pickle_file6:\n",
        "    pickle.dump(smaller_df, output_pickle_file6)\n",
        "\n",
        "with open(output_pickle_file_path7, 'wb') as output_pickle_file7:\n",
        "    pickle.dump(smaller_df, output_pickle_file7)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller fragments of authors-to-co_authors\n",
        "\n",
        "import json\n",
        "import random\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "\n",
        "# Read the JSON data from the file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data = json.load(input_file)\n",
        "\n",
        "\n",
        "# Define the percentage to keep\n",
        "percentage_to_keep = 0.4  # 0.1%\n",
        "\n",
        "# Create a new dictionary to store the sampled data\n",
        "sampled_data = {}\n",
        "\n",
        "# Randomly sample each group of names\n",
        "for key, names in json_data.items():\n",
        "    num_to_keep = int(len(names) * percentage_to_keep)\n",
        "    sampled_data[key] = random.sample(names, num_to_keep)\n",
        "\n",
        "# Save the sampled data to a new JSON file\n",
        "output_json_path = '/content/drive/MyDrive/smallauthor_to_coauthors.json'\n",
        "with open(output_json_path, 'w') as output_file:\n",
        "    json.dump(sampled_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# smaller fragments of author-to-fos and fos-to-author file\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON files\n",
        "input_json_path = '/content/drive/MyDrive/fos_to_authors.json'\n",
        "input_json_path1 = '/content/drive/MyDrive/authors_to_fos.json'\n",
        "\n",
        "# Read the JSON data from the first file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data_fos_to_authors = json.load(input_file)\n",
        "\n",
        "# Read the JSON data from the second file\n",
        "with open(input_json_path1, 'r') as input_file:\n",
        "    json_data_authors_to_fos = json.load(input_file)\n",
        "\n",
        "# Define the percentage to keep\n",
        "percentage_to_keep = 0.4  # 40%\n",
        "\n",
        "# Create new dictionaries to store the sampled data\n",
        "sampled_data_fos_to_authors = {}\n",
        "sampled_data_authors_to_fos = {}\n",
        "\n",
        "# Keep the first 40% of each group of names for the first file\n",
        "for key, names in json_data_fos_to_authors.items():\n",
        "    num_to_keep = int(len(names) * percentage_to_keep)\n",
        "    sampled_data_fos_to_authors[key] = names[:num_to_keep]\n",
        "\n",
        "# Keep the first 40% of each group of names for the second file\n",
        "for key, names in json_data_authors_to_fos.items():\n",
        "    num_to_keep = int(len(names) * percentage_to_keep)\n",
        "    sampled_data_authors_to_fos[key] = names[:num_to_keep]\n",
        "\n",
        "# Save the sampled data to new JSON files\n",
        "output_json_path_fos_to_authors = '/content/drive/MyDrive/smallfos_to_authors.json'\n",
        "with open(output_json_path_fos_to_authors, 'w') as output_file:\n",
        "    json.dump(sampled_data_fos_to_authors, output_file, indent=2)\n",
        "\n",
        "output_json_path_authors_to_fos = '/content/drive/MyDrive/smallauthors_to_fos.json'\n",
        "with open(output_json_path_authors_to_fos, 'w') as output_file:\n",
        "    json.dump(sampled_data_authors_to_fos, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON files\n",
        "input_json_path = '/content/drive/MyDrive/opinion.json'\n",
        "input_json_path1 = '/content/drive/MyDrive/opinionItem.json'\n",
        "\n",
        "# Read the JSON data from the first file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data_opinion = json.load(input_file)\n",
        "\n",
        "# Read the JSON data from the second file\n",
        "with open(input_json_path1, 'r') as input_file:\n",
        "    json_data_opinion_item = json.load(input_file)\n",
        "\n",
        "# Define the percentage to keep\n",
        "percentage_to_keep = 0.4  # 40%\n",
        "\n",
        "# Create new dictionaries to store the sampled data\n",
        "sampled_data_opinion = {}\n",
        "sampled_data_opinion_item = {}\n",
        "\n",
        "# Keep the first 40% of each person's entries for the first file\n",
        "for person, entries in json_data_opinion.items():\n",
        "    num_to_keep = int(len(entries) * percentage_to_keep)\n",
        "    sampled_data_opinion[person] = dict(list(entries.items())[:num_to_keep])\n",
        "\n",
        "# Keep the first 40% of each person's entries for the second file\n",
        "for person, entries in json_data_opinion_item.items():\n",
        "    num_to_keep = int(len(entries) * percentage_to_keep)\n",
        "    sampled_data_opinion_item[person] = dict(list(entries.items())[:num_to_keep])\n",
        "\n",
        "# Save the sampled data to new JSON files\n",
        "output_json_path_opinion = '/content/drive/MyDrive/smallopinion.json'\n",
        "with open(output_json_path_opinion, 'w') as output_file:\n",
        "    json.dump(sampled_data_opinion, output_file, indent=2)\n",
        "\n",
        "output_json_path_opinion_item = '/content/drive/MyDrive/smallopinionItem.json'\n",
        "with open(output_json_path_opinion_item, 'w') as output_file:\n",
        "    json.dump(sampled_data_opinion_item, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "# Specify the path to your input text file\n",
        "input_text_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "input_text_path1 = '/content/drive/MyDrive/testFos.txt'\n",
        "input_text_path2 = '/content/drive/MyDrive/testRating.txt'\n",
        "input_text_path3 = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "input_text_path4 = '/content/drive/MyDrive/trainFos.txt'\n",
        "input_text_path5 = '/content/drive/MyDrive/trainRating.txt'\n",
        "# Specify the path for the new sampled text file\n",
        "output_text_path = '/content/drive/MyDrive/smalltestAuthor.txt'\n",
        "output_text_path1 = '/content/drive/MyDrive/smalltestFos.txt'\n",
        "output_text_path2 = '/content/drive/MyDrive/smalltestRating.txt'\n",
        "output_text_path3 = '/content/drive/MyDrive/smalltrainAuthor.txt'\n",
        "output_text_path4 = '/content/drive/MyDrive/smalltrainFos.txt'\n",
        "output_text_path5 = '/content/drive/MyDrive/smalltrainRating.txt'\n",
        "\n",
        "# Define the percentage to keep (40%)\n",
        "percentage_to_keep = 0.4  # 40%\n",
        "\n",
        "# Read the lines from the input text file\n",
        "with open(input_text_path, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path1, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path2, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path3, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path4, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path5, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "\n",
        "# Calculate the number of names to keep\n",
        "num_names_to_keep = int(len(names) * percentage_to_keep)\n",
        "\n",
        "# Take the first 40% of names\n",
        "sampled_names = names[:num_names_to_keep]\n",
        "\n",
        "# Write the sampled names to the new text file\n",
        "with open(output_text_path, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path1, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path2, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path3, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path4, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "with open(output_text_path5, 'w') as output_file:\n",
        "    for name in sampled_names:\n",
        "        output_file.write(name + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path to your input text file\n",
        "input_text_path = '/content/drive/MyDrive/smalltestAuthor.txt'\n",
        "input_text_path1 = '/content/drive/MyDrive/smalltestFos.txt'\n",
        "input_text_path2 = '/content/drive/MyDrive/smalltestRating.txt'\n",
        "input_text_path3 = '/content/drive/MyDrive/smalltrainAuthor.txt'\n",
        "input_text_path4 = '/content/drive/MyDrive/smalltrainFos.txt'\n",
        "input_text_path5 = '/content/drive/MyDrive/smalltrainRating.txt'\n",
        "\n",
        "# Specify the path for the pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/smalltestAuthor.pkl'\n",
        "output_pickle_path1 = '/content/drive/MyDrive/smalltestFos.pkl'\n",
        "output_pickle_path2 = '/content/drive/MyDrive/smalltestRating.pkl'\n",
        "output_pickle_path3 = '/content/drive/MyDrive/smalltrainAuthor.pkl'\n",
        "output_pickle_path4 = '/content/drive/MyDrive/smalltrainFos.pkl'\n",
        "output_pickle_path5 = '/content/drive/MyDrive/smalltrainRating.pkl'\n",
        "\n",
        "# Read the lines from the input text file\n",
        "with open(input_text_path, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path1, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path2, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path3, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path4, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path5, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "# Create a dictionary with keys as indices and values as names\n",
        "indexed_names = {i: name for i, name in enumerate(names)}\n",
        "\n",
        "# Save the dictionary to a pickle file\n",
        "with open(output_pickle_path, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path1, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path2, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path3, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path4, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path5, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "\n",
        "\n",
        "# convert small training and testing txt files to json format.\n",
        "\n",
        "import json\n",
        "\n",
        "def text_to_json(input_file, output_file):\n",
        "    # Open the text file for reading\n",
        "    with open(input_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    # Convert each line to a floating-point number\n",
        "    numbers = [float(line.strip().rstrip(',')) for line in lines]\n",
        "\n",
        "    # Write the numbers to a JSON file\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(numbers, json_file, indent=2)\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_files = ['/content/drive/MyDrive/smalltrainAuthor.txt', '/content/drive/MyDrive/smalltrainFos.txt', '/content/drive/MyDrive/smalltrainRating.txt',\n",
        "               '/content/drive/MyDrive/smalltestAuthor.txt', '/content/drive/MyDrive/smalltestFos.txt', '/content/drive/MyDrive/smalltestRating.txt']\n",
        "\n",
        "output_files = ['/content/drive/MyDrive/smalltrainAuthor.json', '/content/drive/MyDrive/smalltrainFos.json', '/content/drive/MyDrive/smalltrainRating.json',\n",
        "               '/content/drive/MyDrive/smalltestAuthor.json', '/content/drive/MyDrive/smalltestFos.json', '/content/drive/MyDrive/smalltestRating.json']\n",
        "# Convert each text file to JSON\n",
        "for input_file, output_file in zip(input_files, output_files):\n",
        "    text_to_json(input_file, output_file)\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "0iLSSXz_8R9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gr2amCSmjatW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wi2ODm2hkcZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the input JSON file generated from the first step\n",
        "input_file_path = '/content/drive/MyDrive/weightsFos.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Define the opinion embeddings\n",
        "opinion_embeddings = {\n",
        "    \"0\": (0.0, 0.125),\n",
        "    \"1\": (0.125, 0.25),\n",
        "    \"2\": (0.25, 0.375),\n",
        "    \"3\": (0.375, 0.5),\n",
        "    \"4\": (0.5, 0.625),\n",
        "    \"5\": (0.625, 0.75),\n",
        "    \"6\": (0.75, 0.875),\n",
        "    \"7\": (0.875, 1.0)\n",
        "}\n",
        "\n",
        "# Function to assign opinion based on weight\n",
        "def assign_opinion(weight):\n",
        "    for opinion, (start, end) in opinion_embeddings.items():\n",
        "        if start <= weight <= end:\n",
        "            return opinion\n",
        "    return None\n",
        "\n",
        "# Create a dictionary to store author opinions\n",
        "output_data = {}\n",
        "\n",
        "# Iterate through authors and concepts in the input data\n",
        "for concept, authors in input_data.items():\n",
        "    concept_opinions = {}\n",
        "    for author, weight in authors.items():\n",
        "        opinion = assign_opinion(weight)\n",
        "        if opinion:\n",
        "            concept_opinions[author] = opinion\n",
        "    output_data[concept] = concept_opinions\n",
        "\n",
        "# Save the output data to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinionItem1.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "rukh3iDyKam9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. authors their fos with opinions. (fos-to-authors)\n",
        "\n",
        "import json\n",
        "\n",
        "# Define the opinion embeddings\n",
        "opinion_embeddings = {\n",
        "    \"0\": (0.0, 0.125),\n",
        "    \"1\": (0.125, 0.25),\n",
        "    \"2\": (0.25, 0.375),\n",
        "    \"3\": (0.375, 0.5),\n",
        "    \"4\": (0.5, 0.625),\n",
        "    \"5\": (0.625, 0.75),\n",
        "    \"6\": (0.75, 0.875),\n",
        "    \"7\": (0.875, 1.0)\n",
        "}\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Function to assign opinion based on weight\n",
        "def assign_opinion(weight):\n",
        "    for opinion, (start, end) in opinion_embeddings.items():\n",
        "        if start <= weight <= end:\n",
        "            return opinion\n",
        "    return None\n",
        "\n",
        "# Create a dictionary to store author opinions\n",
        "output_data = {}\n",
        "\n",
        "# Iterate through authors and concepts in the input data\n",
        "for author, concepts in input_data.items():\n",
        "    author_opinions = {}\n",
        "    for concept, weight in concepts.items():\n",
        "        opinion = assign_opinion(weight)\n",
        "        if opinion:\n",
        "            author_opinions[concept] = opinion\n",
        "    output_data[author] = author_opinions\n",
        "\n",
        "# Save the output data to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/opinion.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=2)"
      ],
      "metadata": {
        "id": "EZ1K5kaT6NFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_json_path = '/content/drive/MyDrive/smallauthor_to_coauthors.json'\n",
        "\n",
        "# Read the JSON data from the file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data = json.load(input_file)\n",
        "\n",
        "# Calculate the number of items to keep (40%)\n",
        "percentage_to_keep = 0.4\n",
        "total_items = sum(len(names) for names in json_data.values())\n",
        "num_to_keep = int(total_items * percentage_to_keep)\n",
        "\n",
        "# Create a new dictionary to store the sampled data\n",
        "sampled_data = {}\n",
        "\n",
        "# Iterate through the keys and keep the first 40% of each list\n",
        "current_count = 0\n",
        "for key, names in json_data.items():\n",
        "    sampled_data[key] = names[:num_to_keep - current_count]\n",
        "    current_count += len(sampled_data[key])\n",
        "    if current_count >= num_to_keep:\n",
        "        break\n",
        "\n",
        "# Save the sampled data to a new JSON file\n",
        "with open(output_json_path, 'w') as output_file:\n",
        "    json.dump(sampled_data, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "IAlho3ug8s01"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert authrs to co-authors json to pickle\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/structured_author_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/structured_author_to_coauthors.pkl'\n",
        "\n",
        "# Read the JSON data from the file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    json_data = json.load(input_file)\n",
        "\n",
        "# Save the data to a pickle file\n",
        "with open(output_pickle_path, 'wb') as output_file:\n",
        "    pickle.dump(json_data, output_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "FpmjWltV9pgH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the content from the input file\n",
        "input_filename = \"/content/drive/MyDrive/dblp.txt\"\n",
        "\n",
        "try:\n",
        "    with open(input_filename, 'r') as input_file:\n",
        "        content = input_file.read()\n",
        "        data = json.loads(content)\n",
        "except json.JSONDecodeError as e:\n",
        "    print(f\"Error decoding JSON: {e}\")\n",
        "    data = None\n",
        "\n",
        "if data is not None:\n",
        "    # Organize the data into a structured format (as shown in the previous response)\n",
        "\n",
        "    # Specify the output JSON file\n",
        "    output_filename = \"/content/drive/MyDrive/dblp.json\"\n",
        "\n",
        "    # Write the properly formatted content to the output JSON file\n",
        "    with open(output_filename, 'w') as output_file:\n",
        "        json.dump(data, output_file, indent=2)\n",
        "\n",
        "    print(f\"Conversion successful. Properly formatted JSON saved as '{output_filename}'.\")\n",
        "else:\n",
        "    print(\"Conversion failed due to JSON decoding error.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtK1BVT6vDbM",
        "outputId": "57f73452-e610-4d84-c13d-da513ce63898"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error decoding JSON: Extra data: line 4422 column 2 (char 49914)\n",
            "Conversion failed due to JSON decoding error.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/Attention.py\n",
        "\n"
      ],
      "metadata": {
        "id": "gBRDWAEN-Yry"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/Social_Aggregators.py\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UoemRjMG-dIY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/Social_Encoders.py\n",
        "\n"
      ],
      "metadata": {
        "id": "an7nyEYz-jH8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/UV_Aggregators.py\n"
      ],
      "metadata": {
        "id": "PLaQuY92-llO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "!python /content/drive/MyDrive/GraphRec-WWW19/UV_Encoders.py\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dahwSZK9-n3C"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heMJl3Y6-qzW",
        "outputId": "d182148b-70a6-485b-f1e1-0e35434b0f2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test\", line 201, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/GraphRec-WWW19/Graph_Rec_test\", line 150, in main\n",
            "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size, shuffle=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 349, in __init__\n",
            "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/sampler.py\", line 140, in __init__\n",
            "    raise ValueError(f\"num_samples should be a positive integer value, but got num_samples={self.num_samples}\")\n",
            "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "IO0omPvQnFPU",
        "outputId": "92d0c823-18f6-4b7c-c267-5f913f69a82a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert author_to_co-author in a specific structure\n",
        "\n",
        "import json\n",
        "\n",
        "def convert_to_desired_format(input_file, output_file):\n",
        "    # Load the input JSON file\n",
        "    with open(input_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Create a dictionary to store the converted data\n",
        "    converted_data = {}\n",
        "\n",
        "    # Iterate through each person and their connections\n",
        "    for person, connections in data.items():\n",
        "        # Convert connections to a set of strings\n",
        "        connections_str = \"{%s}\" % \", \".join(map(str, connections))\n",
        "\n",
        "        # Add the person and their connections to the converted data\n",
        "        converted_data[person] = connections_str\n",
        "\n",
        "    # Save the converted data to the output file\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        json.dump(converted_data, output_file, indent=2)\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_path = '/content/drive/MyDrive/author_to_coauthors.json'  # Replace with the actual path to your JSON file\n",
        "output_path = '/content/drive/MyDrive/structured_author_to_coauthors.json'  # Replace with the desired output path\n",
        "\n",
        "# Convert the JSON file to the desired format\n",
        "convert_to_desired_format(input_path, output_path)\n"
      ],
      "metadata": {
        "id": "6GICBK5ESZ9n"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_and_save(input_file, output_file, num_names):\n",
        "    # Read the lines from the input text file\n",
        "    with open(input_file, 'r') as input_file:\n",
        "        names = input_file.read().splitlines()\n",
        "\n",
        "    # Take the first num_names\n",
        "    extracted_names = names[:num_names]\n",
        "\n",
        "    # Save the extracted names to a new text file\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        for name in extracted_names:\n",
        "            output_file.write(name + '\\n')\n",
        "\n",
        "# Specify the paths for input and output files\n",
        "input_path = '/content/drive/MyDrive/testAuthor.txt'\n",
        "output_path = '/content/drive/MyDrive/final50testAuthor.txt'\n",
        "\n",
        "input_path1 = '/content/drive/MyDrive/testFos.txt'\n",
        "output_path1 = '/content/drive/MyDrive/final50testFos.txt'\n",
        "\n",
        "input_path2 = '/content/drive/MyDrive/testRating.txt'\n",
        "output_path2 = '/content/drive/MyDrive/final50testRating.txt'\n",
        "\n",
        "input_path3 = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "output_path3 = '/content/drive/MyDrive/final50trainAuthor.txt'\n",
        "\n",
        "input_path4 = '/content/drive/MyDrive/trainFos.txt'\n",
        "output_path4 = '/content/drive/MyDrive/final50trainFos.txt'\n",
        "\n",
        "input_path5 = '/content/drive/MyDrive/trainRating.txt'\n",
        "output_path5 = '/content/drive/MyDrive/final50trainRating.txt'\n",
        "\n",
        "# Extract and save the first 50 names to a new text file\n",
        "extract_and_save(input_path, output_path, 50)\n",
        "extract_and_save(input_path1, output_path1, 50)\n",
        "extract_and_save(input_path2, output_path2, 50)\n",
        "extract_and_save(input_path3, output_path3, 50)\n",
        "extract_and_save(input_path4, output_path4, 50)\n",
        "extract_and_save(input_path5, output_path5, 50)\n"
      ],
      "metadata": {
        "id": "RBv5TWsGuNzi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-qvAqpUkUwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23TOba33L4qf"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Create a dictionary to store authors for each FOS with aggregated weights\n",
        "author_fos_dict = {}\n",
        "\n",
        "# Specify the input file path\n",
        "input_file_path = '/content/drive/MyDrive/dblp_papers_v11.txt'  # Replace with your file path\n",
        "\n",
        "# Read and process each line in the input file\n",
        "with open(input_file_path, 'r') as file:\n",
        "    for line in file:\n",
        "        # Load each line as a JSON object\n",
        "        paper = json.loads(line)\n",
        "\n",
        "        authors = paper.get('authors', [])\n",
        "        fos_list = paper.get('fos', [])\n",
        "\n",
        "        # Iterate through FOS for each paper\n",
        "        for fos in fos_list:\n",
        "            fos_name = fos['name']\n",
        "            fos_weight = fos['w']\n",
        "\n",
        "            if fos_name not in author_fos_dict:\n",
        "                author_fos_dict[fos_name] = {}\n",
        "\n",
        "            # Create a dictionary to store aggregated weights for each author for this FOS\n",
        "            aggregated_weights = {}\n",
        "\n",
        "            # Iterate through authors for the paper and aggregate weights\n",
        "            for author in authors:\n",
        "                author_name = author['name']\n",
        "\n",
        "                # Aggregate weights for the same author for this FOS\n",
        "                if author_name in aggregated_weights:\n",
        "                    aggregated_weights[author_name].append(fos_weight)\n",
        "                else:\n",
        "                    aggregated_weights[author_name] = [fos_weight]\n",
        "\n",
        "            # Calculate the average weights for each author for this FOS\n",
        "            for author, weights in aggregated_weights.items():\n",
        "                average_weight = sum(weights) / len(weights)\n",
        "                author_fos_dict[fos_name][author] = average_weight\n",
        "\n",
        "# Save the author FOS data with aggregated weights to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/weightsFos.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_fos_dict, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "BNbVEsmb_Xp4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the file paths for the JSON and TXT files\n",
        "\n",
        "json_file_paths = ['/content/drive/MyDrive/final50authors_to_fos.json', '/content/drive/MyDrive/final50opinion.json',\n",
        "              '/content/drive/MyDrive/final50fos_to_authors.json', '/content/drive/MyDrive/final50opinionItem1.json']\n",
        "# txt_file_paths = ['/content/drive/MyDrive/trainAuthor.txt', '/content/drive/MyDrive/trainFos.txt', '/content/drive/MyDrive/trainRating.txt',\n",
        "#                   '/content/drive/MyDrive/testAuthor.txt', '/content/drive/MyDrive/testFos.txt', '/content/drive/MyDrive/testRating.txt']\n",
        "\n",
        "\n",
        "# Specify the output file path for the combined data\n",
        "output_file_path = '/content/drive/MyDrive/final50combinedFile.json'\n",
        "\n",
        "# Initialize an empty list to store individual file data\n",
        "all_data = []\n",
        "\n",
        "# Read data from JSON files\n",
        "for json_path in json_file_paths:\n",
        "    with open(json_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    all_data.append(data)\n",
        "\n",
        "# Read data from TXT files\n",
        "# for txt_path in txt_file_paths:\n",
        "#     with open(txt_path, 'r') as txt_file:\n",
        "#         data = {'text_data': txt_file.read()}\n",
        "#     all_data.append(data)\n",
        "\n",
        "\n",
        "\n",
        "# Write the combined data to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(all_data, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "ir7CW-Gkhm2d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- DATA MODIFICATION STARTS ----------------- UNIQUE VALUE CONVERSION -----------\n",
        "\n",
        "import json\n",
        "\n",
        "def assign_unique_ids(input_data):\n",
        "    # Create dictionaries to store unique IDs for authors and values\n",
        "    author_id_mapping = {}\n",
        "    value_id_mapping = {}\n",
        "\n",
        "    # Convert original data to use unique IDs and keep only the first occurrence of each value\n",
        "    converted_data = {}\n",
        "    for idx, (author, values) in enumerate(input_data.items()):\n",
        "        # Assign unique ID to author\n",
        "        author_id_mapping[author] = idx\n",
        "\n",
        "        # Process values and keep only the first occurrence\n",
        "        unique_values = []\n",
        "        seen_values = set()\n",
        "        for value in values:\n",
        "            if value not in seen_values:\n",
        "                unique_values.append(value)\n",
        "                seen_values.add(value)\n",
        "\n",
        "        # Assign unique IDs to values\n",
        "        converted_data[author_id_mapping[author]] = [value_id_mapping.setdefault(value, len(value_id_mapping)) for value in unique_values]\n",
        "\n",
        "    return converted_data\n",
        "\n",
        "# Read data from JSON input file\n",
        "input_file_path = '/content/drive/MyDrive/authors_to_fos.json'  # Replace with your actual input file path\n",
        "with open(input_file_path, 'r') as file:\n",
        "    input_data = json.load(file)\n",
        "\n",
        "# Perform the conversion\n",
        "result_data = assign_unique_ids(input_data)\n",
        "\n",
        "# Write result to JSON output file\n",
        "output_file_path = '/content/drive/MyDrive/unique_authors_to_fos.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as file:\n",
        "    json.dump(result_data, file, indent=2)\n",
        "\n",
        "print(\"Conversion completed. Result saved to\", output_file_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNufziR8m2n-",
        "outputId": "242f6dc8-474a-4665-b574-b17414103ba0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conversion completed. Result saved to /content/drive/MyDrive/unique_authors_to_fos.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- DATA MODIFICATION STARTS ----------------- UNIQUE VALUE CONVERSION -----------FOS_TO_AUTHORS\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_authors_to_fos.json'\n",
        "\n",
        "# Read input JSON from the file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Create a new dictionary for the desired output\n",
        "output_dict = {}\n",
        "\n",
        "# Iterate through the input data and populate the output dictionary\n",
        "for key, values in data.items():\n",
        "    for value in values:\n",
        "        int_value = int(value)\n",
        "        if int_value not in output_dict:\n",
        "            output_dict[int_value] = []\n",
        "        output_dict[int_value].append(int(key))\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_file_path = '/content/drive/MyDrive/unique_fos_to_authors.json'\n",
        "\n",
        "# Write the output JSON to a file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_dict, output_file, indent=2)\n",
        "\n",
        "print(f\"Output written to {output_file_path}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcchEyt2ybo5",
        "outputId": "f21eae06-9bd9-4b48-a09c-c304e1661c7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output written to /content/drive/MyDrive/unique_fos_to_authors.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- DATA MODIFICATION STARTS ----------------- UNIQUE VALUE CONVERSION -----------AUTHOR_TO_COAUTHORS\n",
        "\n",
        "import json\n",
        "\n",
        "# Create a dictionary to store authors and their co-authors\n",
        "author_to_coauthors = {}\n",
        "author_id_mapping = {}\n",
        "\n",
        "# Function to add co-authors for an author\n",
        "def add_coauthors(authors):\n",
        "    for author in authors:\n",
        "        if author not in author_to_coauthors:\n",
        "            author_to_coauthors[author] = set()  # Use a set to ensure uniqueness\n",
        "\n",
        "        author_to_coauthors[author].update(authors)\n",
        "\n",
        "# Read the text file\n",
        "with open('/content/drive/MyDrive/dblp_papers_v11.txt', 'r') as file:\n",
        "    for line in file:\n",
        "        data = json.loads(line)\n",
        "\n",
        "        # Extract authors for the current paper\n",
        "        authors = [author_info['name'] for author_info in data.get('authors', [])]\n",
        "\n",
        "        # Add co-authors for the current paper\n",
        "        add_coauthors(authors)\n",
        "\n",
        "# Assign unique IDs to authors\n",
        "for idx, author in enumerate(author_to_coauthors.keys()):\n",
        "    author_id_mapping[author] = idx\n",
        "\n",
        "# Create a dictionary to store authors' IDs and their co-authors' IDs\n",
        "network_with_ids = {}\n",
        "for author, coauthors in author_to_coauthors.items():\n",
        "    author_id = author_id_mapping[author]\n",
        "    coauthor_ids = {author_id_mapping[coauthor] for coauthor in coauthors if coauthor != author}  # Exclude self-references\n",
        "    network_with_ids[author_id] = f\"{{{', '.join(map(str, coauthor_ids))}}}\"\n",
        "\n",
        "# Define the output file path\n",
        "output_file_path = '/content/drive/MyDrive/unique_author_to_coauthors.json'\n",
        "\n",
        "# Save the author network with IDs to a JSON file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(network_with_ids, output_file, indent=2)\n",
        "\n",
        "print(f\"Author network with IDs (self-references removed) saved\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8v6hSu8KgbR",
        "outputId": "d5f04bfd-481f-4c6a-f706-2bf0da2f9484"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author network with IDs (self-references removed) saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- DATA MODIFICATION STARTS ----------------- UNIQUE VALUE CONVERSION -----------OPINIONITEM\n",
        "\n",
        "import json\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Create dictionaries to store unique IDs for authors and concepts\n",
        "author_id_mapping = {author: idx for idx, author in enumerate(input_data.keys())}\n",
        "concept_id_mapping = {concept: idx for concepts in input_data.values() for idx, concept in enumerate(concepts.keys())}\n",
        "\n",
        "# Function to assign opinion based on weight\n",
        "def assign_opinion(weight):\n",
        "    for opinion, (start, end) in opinion_embeddings.items():\n",
        "        if start <= weight <= end:\n",
        "            return int(opinion)\n",
        "    return None\n",
        "\n",
        "# Create a dictionary to store author opinions with unique IDs\n",
        "output_data = {}\n",
        "\n",
        "# Iterate through authors and concepts in the input data\n",
        "for author, concepts in input_data.items():\n",
        "    author_id = author_id_mapping[author]\n",
        "    author_opinions = []\n",
        "    for concept, weight in concepts.items():\n",
        "        concept_id = concept_id_mapping[concept]\n",
        "        opinion = assign_opinion(weight)\n",
        "        if opinion is not None:\n",
        "            author_opinions.append(opinion)\n",
        "    output_data[author_id] = author_opinions\n",
        "\n",
        "# Save the output data to a JSON file with only weights\n",
        "output_file_path = '/content/drive/MyDrive/unique_opinionItem.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fbF28bAUWH_W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- DATA MODIFICATION STARTS ----------------- UNIQUE VALUE CONVERSION -----------OPINION\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_opinionItem.json'\n",
        "\n",
        "# Read input JSON from the file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Create a new dictionary for the desired output\n",
        "output_dict = {}\n",
        "\n",
        "# Iterate through the input data and populate the output dictionary\n",
        "for key, values in data.items():\n",
        "    for value in values:\n",
        "        int_value = int(value)\n",
        "        if int_value not in output_dict:\n",
        "            output_dict[int_value] = []\n",
        "        output_dict[int_value].append(int(key))\n",
        "\n",
        "# Specify the path for the output JSON file\n",
        "output_file_path = '/content/drive/MyDrive/unique_opinion.json'\n",
        "\n",
        "# Write the output JSON to a file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(output_dict, output_file, indent=2)\n",
        "\n",
        "print(f\"Output written to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-NCyzvPWxac",
        "outputId": "a3af0c0e-6484-449c-8497-7a451f228a1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output written to /content/drive/MyDrive/unique_opinion.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------  NEW METHOD TO GENERATE FILES WITH WEIGHTS ------------------------\n",
        "\n",
        "import json\n",
        "\n",
        "# Define the opinion embeddings\n",
        "opinion_embeddings = {\n",
        "    \"0\": (0.0, 0.125),\n",
        "    \"1\": (0.125, 0.25),\n",
        "    \"2\": (0.25, 0.375),\n",
        "    \"3\": (0.375, 0.5),\n",
        "    \"4\": (0.5, 0.625),\n",
        "    \"5\": (0.625, 0.75),\n",
        "    \"6\": (0.75, 0.875),\n",
        "    \"7\": (0.875, 1.0)\n",
        "}\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Function to assign opinion based on weight\n",
        "def assign_opinion(weight):\n",
        "    for opinion, (start, end) in opinion_embeddings.items():\n",
        "        if start <= weight <= end:\n",
        "            return opinion\n",
        "    return None\n",
        "\n",
        "# Create dictionaries to store data for the first and second JSON files\n",
        "output_data_1 = {}\n",
        "output_data_2 = {}\n",
        "\n",
        "# Iterate through authors and concepts in the input data\n",
        "for author, concepts in input_data.items():\n",
        "    author_opinions = {}\n",
        "    for concept, weight in concepts.items():\n",
        "        # Assign opinion based on weight\n",
        "        opinion = assign_opinion(weight)\n",
        "        if opinion:\n",
        "            # Create unique IDs for fos and authors\n",
        "            fos_id = f\"{concept}\"\n",
        "            author_id = f\"{author}\"\n",
        "\n",
        "            # Update output_data_1 with fos_id and weights\n",
        "            if fos_id not in output_data_1:\n",
        "                output_data_1[fos_id] = [int(opinion)]\n",
        "            else:\n",
        "                output_data_1[fos_id].append(int(opinion))\n",
        "\n",
        "            # Update output_data_2 with author_id and fos_id\n",
        "            if author_id not in output_data_2:\n",
        "                output_data_2[author_id] = [int(opinion)]\n",
        "            else:\n",
        "                output_data_2[author_id].append(int(opinion))\n",
        "\n",
        "# Save the output data to the first JSON file\n",
        "output_file_path_1 = '/content/drive/MyDrive/unique_opinionItem.json'  # Replace with your desired output file path\n",
        "with open(output_file_path_1, 'w') as output_file_1:\n",
        "    json.dump(output_data_1, output_file_1, indent=2)\n",
        "\n",
        "# Save the output data to the second JSON file\n",
        "output_file_path_2 = '/content/drive/MyDrive/unique_opinion.json'  # Replace with your desired output file path\n",
        "with open(output_file_path_2, 'w') as output_file_2:\n",
        "    json.dump(output_data_2, output_file_2, indent=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "u7ReY2ag4lwJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------- assign unique id -------------- unique_opinion.json\n",
        "\n",
        "import json\n",
        "\n",
        "# Path to the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_opinion.json'  # Replace with the path to your input JSON file\n",
        "\n",
        "# Load input JSON from file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Create a dictionary to map unique numerical IDs to author names\n",
        "author_id_mapping = {}\n",
        "current_id = 0\n",
        "\n",
        "# Modify the input data to use unique IDs for author names\n",
        "for author_name in input_data:\n",
        "    author_id_mapping[str(current_id)] = author_data = input_data[author_name]\n",
        "    current_id += 1\n",
        "\n",
        "# Save the modified data to a new JSON file\n",
        "output_file_path = '/content/drive/MyDrive/uniqueId_opinion.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_id_mapping, output_file, indent=2)\n",
        "\n",
        "print(\"Modified JSON with unique author IDs saved to:\", output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kN-NBzhUeZPW",
        "outputId": "3bc4426e-16be-4860-cf07-4782a8b50090"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified JSON with unique author IDs saved to: /content/drive/MyDrive/uniqueId_opinion.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------------- assign unique id -------------- unique_opinionItem.json\n",
        "\n",
        "import json\n",
        "\n",
        "# Path to the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_opinionItem.json'  # Replace with the path to your input JSON file\n",
        "\n",
        "# Load input JSON from file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Create a dictionary to map unique numerical IDs to author names\n",
        "author_id_mapping = {}\n",
        "current_id = 0\n",
        "\n",
        "# Modify the input data to use unique IDs for author names\n",
        "for author_name in input_data:\n",
        "    author_id_mapping[str(current_id)] = author_data = input_data[author_name]\n",
        "    current_id += 1\n",
        "\n",
        "# Save the modified data to a new JSON file\n",
        "output_file_path = '/content/drive/MyDrive/uniqueId_opinionItem.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(author_id_mapping, output_file, indent=2)\n",
        "\n",
        "print(\"Modified JSON with unique author IDs saved to:\", output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSEJ9i0uevDr",
        "outputId": "e2c1f635-aba8-47d9-cfec-170303c3dac9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified JSON with unique author IDs saved to: /content/drive/MyDrive/uniqueId_opinionItem.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#combine training, testing and other files unique files into 1 json file\n",
        "\n",
        "import json\n",
        "\n",
        "# Read content from file1.json\n",
        "with open('/content/drive/MyDrive/unique_trainAuthor.json', 'r') as file1:\n",
        "    content_file1 = json.load(file1)\n",
        "\n",
        "# Read content from file2.json\n",
        "with open('/content/drive/MyDrive/unique_trainFos.json', 'r') as file2:\n",
        "    content_file2 = json.load(file2)\n",
        "\n",
        "# Read content from file3.json\n",
        "with open('/content/drive/MyDrive/unique_ratings_train.json', 'r') as file3:\n",
        "    content_file3 = json.load(file3)\n",
        "\n",
        "with open('/content/drive/MyDrive/unique_testAuthor.json', 'r') as file4:\n",
        "    content_file4 = json.load(file4)\n",
        "\n",
        "# Read content from file2.json\n",
        "with open('/content/drive/MyDrive/unique_testFos.json', 'r') as file5:\n",
        "    content_file5 = json.load(file5)\n",
        "\n",
        "# Read content from file3.json\n",
        "with open('/content/drive/MyDrive/unique_ratings_test.json', 'r') as file6:\n",
        "    content_file6 = json.load(file6)\n",
        "\n",
        "# Read content from file4.json\n",
        "with open('/content/drive/MyDrive/unique_author_to_coauthors.json', 'r') as file7:\n",
        "    content_file7 = json.load(file7)\n",
        "\n",
        "# Read content from file4.json\n",
        "with open('/content/drive/MyDrive/embedding.json', 'r') as file8:\n",
        "    content_file8 = json.load(file8)\n",
        "\n",
        "# Combine the content into a list\n",
        "combined_content = [content_file1, content_file2, content_file3, content_file4, content_file5, content_file6, content_file7, content_file8 ]\n",
        "\n",
        "# Write the combined content to combined.json\n",
        "with open('/content/drive/MyDrive/combinedTTFiles.json', 'w') as combined_file:\n",
        "    json.dump(combined_content, combined_file, indent=2)\n"
      ],
      "metadata": {
        "id": "3HHLQo5MRl7m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine 2 final important json files\n",
        "import json\n",
        "\n",
        "def combine_json_files(file1_path, file2_path, output_path):\n",
        "    # Read File 1\n",
        "    with open(file1_path, 'r') as file1:\n",
        "        file1_structure = json.load(file1)\n",
        "\n",
        "    # Read File 2\n",
        "    with open(file2_path, 'r') as file2:\n",
        "        file2_structure = json.load(file2)\n",
        "\n",
        "    # Combine structures\n",
        "    combined_structure = file1_structure + file2_structure\n",
        "\n",
        "    # Write to an output JSON file\n",
        "    with open(output_path, 'w') as output_file:\n",
        "        json.dump(combined_structure, output_file, indent=2)\n",
        "\n",
        "    print(f\"Combined JSON file created successfully at {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "file1_path = '/content/drive/MyDrive/unique_combinedFile.json'\n",
        "file2_path = '/content/drive/MyDrive/combinedTTFiles.json'\n",
        "output_path = '/content/drive/MyDrive/Final_combinedFile.json'\n",
        "\n",
        "combine_json_files(file1_path, file2_path, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmmEGRCxk2Tu",
        "outputId": "a94c23b2-ca5f-4996-cb97-222159576a91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined JSON file created successfully at /content/drive/MyDrive/Final_combinedFile.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to your input text file\n",
        "input_text_path = '/content/drive/MyDrive/trainAuthor.txt'\n",
        "input_text_path1 = '/content/drive/MyDrive/trainFos.txt'\n",
        "input_text_path2 = '/content/drive/MyDrive/trainRating.txt'\n",
        "input_text_path3 = '/content/drive/MyDrive/testAuthor.txt'\n",
        "input_text_path4 = '/content/drive/MyDrive/testFos.txt'\n",
        "input_text_path5 = '/content/drive/MyDrive/testRating.txt'\n",
        "\n",
        "# Specify the path for the new sampled text file\n",
        "output_text_path = '/content/drive/MyDrive/50trainAuthor.txt'\n",
        "output_text_path1 = '/content/drive/MyDrive/50trainFos.txt'\n",
        "output_text_path2 = '/content/drive/MyDrive/50trainRating.txt'\n",
        "output_text_path3 = '/content/drive/MyDrive/50testAuthor.txt'\n",
        "output_text_path4 = '/content/drive/MyDrive/50testFos.txt'\n",
        "output_text_path5 = '/content/drive/MyDrive/50testRating.txt'\n",
        "\n",
        "# Define the number of lines to keep (50)\n",
        "num_lines_to_keep = 51\n",
        "\n",
        "# Read the lines from the input text file\n",
        "with open(input_text_path, 'r') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "with open(input_text_path1, 'r') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "with open(input_text_path2, 'r') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "with open(input_text_path3, 'r') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "with open(input_text_path4, 'r') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "with open(input_text_path5, 'r') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Take the first 50 lines\n",
        "sampled_lines = lines[:num_lines_to_keep]\n",
        "\n",
        "# Write the sampled lines to the new text file\n",
        "\n",
        "\n",
        "with open(output_text_path, 'w') as output_file:\n",
        "    output_file.write(''.join(sampled_lines))\n",
        "    output_file.write(']')\n",
        "\n",
        "\n",
        "with open(output_text_path1, 'w') as output_file:\n",
        "    output_file.write(''.join(sampled_lines))\n",
        "    output_file.write(']')\n",
        "\n",
        "\n",
        "with open(output_text_path2, 'w') as output_file:\n",
        "    output_file.write(''.join(sampled_lines))\n",
        "    output_file.write(']')\n",
        "\n",
        "\n",
        "with open(output_text_path3, 'w') as output_file:\n",
        "    output_file.write(''.join(sampled_lines))\n",
        "    output_file.write(']')\n",
        "\n",
        "\n",
        "with open(output_text_path4, 'w') as output_file:\n",
        "    output_file.write(''.join(sampled_lines))\n",
        "    output_file.write(']')\n",
        "\n",
        "\n",
        "with open(output_text_path5, 'w') as output_file:\n",
        "    output_file.write(''.join(sampled_lines))\n",
        "    output_file.write(']')\n",
        "\n"
      ],
      "metadata": {
        "id": "Svmf9uIw_4xS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Function to save data to a pickle file\n",
        "def save_to_pickle(data, output_file_path):\n",
        "    with open(output_file_path, 'wb') as output_file:\n",
        "        pickle.dump(data, output_file)\n",
        "\n",
        "output_text_path = '/content/drive/MyDrive/50trainAuthor.txt'\n",
        "output_text_path1 = '/content/drive/MyDrive/50trainFos.txt'\n",
        "output_text_path2 = '/content/drive/MyDrive/50trainRating.txt'\n",
        "output_text_path3 = '/content/drive/MyDrive/50testAuthor.txt'\n",
        "output_text_path4 = '/content/drive/MyDrive/50testFos.txt'\n",
        "output_text_path5 = '/content/drive/MyDrive/50testRating.txt'\n",
        "\n",
        "# Specify the path for the new pickle files\n",
        "output_pickle_path = '/content/drive/MyDrive/50trainAuthor.pkl'\n",
        "output_pickle_path1 = '/content/drive/MyDrive/50trainFos.pkl'\n",
        "output_pickle_path2 = '/content/drive/MyDrive/50trainRating.pkl'\n",
        "output_pickle_path3 = '/content/drive/MyDrive/50testAuthor.pkl'\n",
        "output_pickle_path4 = '/content/drive/MyDrive/50testFos.pkl'\n",
        "output_pickle_path5 = '/content/drive/MyDrive/50testRating.pkl'\n",
        "\n",
        "# Load the sampled lines from the text files\n",
        "with open(output_text_path, 'r') as output_file:\n",
        "    sampled_lines = output_file.read()\n",
        "\n",
        "with open(output_text_path1, 'r') as output_file:\n",
        "    sampled_lines = output_file.read()\n",
        "\n",
        "with open(output_text_path2, 'r') as output_file:\n",
        "    sampled_lines = output_file.read()\n",
        "\n",
        "with open(output_text_path3, 'r') as output_file:\n",
        "    sampled_lines = output_file.read()\n",
        "\n",
        "with open(output_text_path4, 'r') as output_file:\n",
        "    sampled_lines = output_file.read()\n",
        "\n",
        "with open(output_text_path5, 'r') as output_file:\n",
        "    sampled_lines = output_file.read()\n",
        "\n",
        "\n",
        "# Convert the sampled lines to a list (assuming it is a list)\n",
        "sampled_data = eval(sampled_lines)\n",
        "\n",
        "# Save the data to pickle files\n",
        "save_to_pickle(sampled_data, output_pickle_path)\n",
        "save_to_pickle(sampled_data, output_pickle_path1)\n",
        "save_to_pickle(sampled_data, output_pickle_path2)\n",
        "save_to_pickle(sampled_data, output_pickle_path3)\n",
        "save_to_pickle(sampled_data, output_pickle_path4)\n",
        "save_to_pickle(sampled_data, output_pickle_path5)\n"
      ],
      "metadata": {
        "id": "cCBNv5aDFFRj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# convert json to pickle\n",
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_combinedFile.json'\n",
        "\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/unique_combinedFile.pkl'\n",
        "\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(input_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n"
      ],
      "metadata": {
        "id": "IYE8NiUTFpmd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the path to the input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/author_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the new JSON file\n",
        "output_json_path = '/content/drive/MyDrive/50author_to_coauthors.json'\n",
        "\n",
        "# Read the content from the input JSON file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    data = json.load(input_file)\n",
        "\n",
        "# Extract only two objects and their values\n",
        "extracted_data = {key: data[key] for key in list(data.keys())[:51]}\n",
        "\n",
        "# Write the extracted data to the new JSON file\n",
        "with open(output_json_path, 'w') as output_file:\n",
        "    json.dump(extracted_data, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "ndIWo-R_He4e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# convert json to pickle\n",
        "\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_author_to_coauthors.json'\n",
        "\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path = '/content/drive/MyDrive/unique_author_to_coauthors.pkl'\n",
        "\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(input_file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n"
      ],
      "metadata": {
        "id": "2dXwYpRgHpmX"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Specify the path to your input JSON file\n",
        "input_json_path = '/content/drive/MyDrive/50author_to_coauthors.json'\n",
        "\n",
        "# Specify the path for the new JSON file\n",
        "output_json_path = '/content/drive/MyDrive/50processed_author_to_coauthors.json'\n",
        "\n",
        "# Function to convert JSON structure to the requested format\n",
        "def convert_to_new_structure(input_data):\n",
        "    output_data = {}\n",
        "\n",
        "    for key, value in input_data.items():\n",
        "        # Filter out neighbors that are not present in the keys\n",
        "        neighbors = [str(list(input_data.keys()).index(neighbor)) for neighbor in value if neighbor in input_data.keys()]\n",
        "        output_data[str(list(input_data.keys()).index(key))] = \"{\" + \", \".join(neighbors) + \"}\"\n",
        "\n",
        "    return output_data\n",
        "\n",
        "# Read the input JSON file\n",
        "with open(input_json_path, 'r') as input_file:\n",
        "    input_json = json.load(input_file)\n",
        "\n",
        "# Convert to the new structure\n",
        "output_json = convert_to_new_structure(input_json)\n",
        "\n",
        "# Write the result to the output JSON file\n",
        "with open(output_json_path, 'w') as output_file:\n",
        "    json.dump(output_json, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "DZLjvTLdPTjs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACT 50\n",
        "import json\n",
        "\n",
        "def extract_and_save(input_file, output_file, num_names=3):\n",
        "    # Load the input JSON file\n",
        "    with open(input_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Extract the starting 'num_names' names and their lists\n",
        "    extracted_data = list(data.items())[:num_names]\n",
        "\n",
        "    # Convert the extracted data to a list of dictionaries\n",
        "    output_data = [{name: connections} for name, connections in extracted_data]\n",
        "\n",
        "    # Save the extracted data to the output file\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        json.dump(output_data, output_file, indent=2)\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_path = '/content/drive/MyDrive/authors_to_fos.json'\n",
        "input_path1 = '/content/drive/MyDrive/fos_to_authors.json'  # Replace with the actual path to your JSON file\n",
        "output_path = '/content/drive/MyDrive/50authors_to_fos.json'\n",
        "output_path1 = '/content/drive/MyDrive/50fos_to_authors.json'  # Replace with the desired output path\n",
        "\n",
        "# Set the number of names to extract (in this case, 50)\n",
        "num_names_to_extract = 50\n",
        "\n",
        "# Extract and save the data to a new JSON file\n",
        "extract_and_save(input_path, output_path, num_names_to_extract)\n",
        "extract_and_save(input_path1, output_path1, num_names_to_extract)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Q-jwWAuV_BTI",
        "outputId": "6d2edde7-4d18-4df4-b465-de2f449f3e4b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-575b85fd578e>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Extract and save the data to a new JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mextract_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_names_to_extract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mextract_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_path1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_names_to_extract\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-575b85fd578e>\u001b[0m in \u001b[0;36mextract_and_save\u001b[0;34m(input_file, output_file, num_names)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Load the input JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Extract the starting 'num_names' names and their lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def extract_and_save(input_file, output_file, num_entries=3):\n",
        "    # Load the input JSON file\n",
        "    with open(input_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Extract the starting 'num_entries' entries from the dictionary\n",
        "    extracted_data = dict(list(data.items())[:num_entries])\n",
        "\n",
        "    # Save the extracted data to the output file\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        json.dump(extracted_data, output_file, indent=2)\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_path = '/content/drive/MyDrive/opinion.json'\n",
        "input_path1 = '/content/drive/MyDrive/opinionItem1.json'  # Replace with the actual path to your JSON file\n",
        "output_path = '/content/drive/MyDrive/50opinion.json'\n",
        "output_path1 = '/content/drive/MyDrive/50opinionItem1.json'  # Replace with the desired output path\n",
        "\n",
        "# Set the number of entries to extract (in this case, 50)\n",
        "num_entries_to_extract = 50\n",
        "\n",
        "# Extract and save the data to a new JSON file\n",
        "extract_and_save(input_path, output_path, num_entries_to_extract)\n",
        "extract_and_save(input_path1, output_path1, num_entries_to_extract)\n"
      ],
      "metadata": {
        "id": "Gi2WJOd2Hxuz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dictionary for combined file --- 1.  author_to_fos and fos_to_authors\n",
        "\n",
        "import json\n",
        "\n",
        "def process_data(input_file, output_file):\n",
        "    # Read data from the input JSON file\n",
        "    with open(input_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Process the data and create a dictionary with authors as keys and their publications as values\n",
        "    result_dict = {}\n",
        "    for entry in data:\n",
        "        author_name, publications = entry.popitem()\n",
        "        result_dict[author_name] = publications\n",
        "\n",
        "    # Write the processed data to the output JSON file\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        json.dump(result_dict, output_file, indent=2)\n",
        "\n",
        "# Specify input and output file paths\n",
        "input_path = '/content/drive/MyDrive/50fos_to_authors.json'  # Replace with the actual path to your input JSON file\n",
        "output_path = '/content/drive/MyDrive/final50fos_to_authors.json'  # Replace with the desired output path\n",
        "\n",
        "# Process the data and save to the output JSON file\n",
        "process_data(input_path, output_path)\n"
      ],
      "metadata": {
        "id": "AJOQ27IcHvqd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dictionary for combined file --- 2.  opinions and opinionItem\n",
        "\n",
        "import json\n",
        "\n",
        "def process_data(input_file, output_file):\n",
        "    with open(input_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    output_data = {}\n",
        "    for author, publications in data.items():\n",
        "        output_data[author] = [rating for publication, rating in publications.items()]\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(output_data, f, indent=2)\n",
        "\n",
        "# Replace 'input.json' and 'output.json' with your actual input and output file paths\n",
        "process_data('/content/drive/MyDrive/50opinionItem1.json', '/content/drive/MyDrive/final50opinionItem1.json')\n"
      ],
      "metadata": {
        "id": "ZAWoIPi9TJSU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dictionary for combined file --- 3.  authors - to- co_authors\n",
        "\n",
        "import json\n",
        "\n",
        "# Replace 'input.json' with your actual input file path\n",
        "with open('/content/drive/MyDrive/50author_to_coauthors.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "output_data = {}\n",
        "\n",
        "for key, values in data.items():\n",
        "    output_data[key] = \"{\" + \", \".join(values) + \"}\"\n",
        "\n",
        "# Replace 'output.json' with your desired output file path\n",
        "with open('/content/drive/MyDrive/final50author_to_coauthors.json', 'w') as file:\n",
        "    json.dump(output_data, file, indent=2)\n"
      ],
      "metadata": {
        "id": "RxSObE1oeVkw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "# Specify the path to your input text file\n",
        "input_text_path = '/content/drive/MyDrive/final50_trainAuthor.txt'\n",
        "input_text_path1 = '/content/drive/MyDrive/final50_trainFos.txt'\n",
        "input_text_path2 = '/content/drive/MyDrive/final50_trainRating.txt'\n",
        "input_text_path3 = '/content/drive/MyDrive/final50_testAuthor.txt'\n",
        "input_text_path4 = '/content/drive/MyDrive/final50_testFos.txt'\n",
        "input_text_path5 = '/content/drive/MyDrive/final50_testRating.txt'\n",
        "\n",
        "# Specify the path for the pickle file\n",
        "output_pickle_path = '/content/drive/MyDrive/final50_trainAuthor.pkl'\n",
        "output_pickle_path1 = '/content/drive/MyDrive/final50_trainFos.pkl'\n",
        "output_pickle_path2 = '/content/drive/MyDrive/final50_trainRating.pkl'\n",
        "output_pickle_path3 = '/content/drive/MyDrive/final50_testAuthor.pkl'\n",
        "output_pickle_path4 = '/content/drive/MyDrive/final50_testFos.pkl'\n",
        "output_pickle_path5 = '/content/drive/MyDrive/final50_testRating.pkl'\n",
        "\n",
        "# Read the lines from the input text file\n",
        "with open(input_text_path, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path1, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path2, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path3, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path4, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "with open(input_text_path5, 'r') as input_file:\n",
        "    names = input_file.read().splitlines()\n",
        "\n",
        "# Create a dictionary with keys as indices and values as names\n",
        "indexed_names = {i: name for i, name in enumerate(names)}\n",
        "\n",
        "# Save the dictionary to a pickle file\n",
        "with open(output_pickle_path, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path1, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path2, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path3, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path4, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n",
        "\n",
        "with open(output_pickle_path5, 'wb') as output_file:\n",
        "    pickle.dump(indexed_names, output_file)\n"
      ],
      "metadata": {
        "id": "6HmVCKoQLGAX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract 50 from training dataset:\n",
        "\n",
        "import json\n",
        "\n",
        "def extract_and_write(input_file, output_file):\n",
        "    with open(input_file, 'r') as input_file:\n",
        "        text = input_file.read()\n",
        "\n",
        "    # Remove commas\n",
        "    text = text.replace(',', '')\n",
        "\n",
        "    lines = text.strip().split('\\n')\n",
        "    first_5_entries = lines[:50]\n",
        "\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        json.dump(first_5_entries, output_file, indent=2)\n",
        "\n",
        "# Input file names\n",
        "input_file_names = [\n",
        "    '/content/drive/MyDrive/trainAuthor.txt',\n",
        "    '/content/drive/MyDrive/trainFos.txt',\n",
        "    '/content/drive/MyDrive/trainRating.txt'\n",
        "]\n",
        "\n",
        "# Output file names\n",
        "output_file_names = [\n",
        "    '/content/drive/MyDrive/final50_trainAuthor.json',\n",
        "    '/content/drive/MyDrive/final50_trainFos.json',\n",
        "    '/content/drive/MyDrive/final50_trainRating.json'\n",
        "]\n",
        "\n",
        "# Extract and write for each input-output pair\n",
        "for i in range(3):\n",
        "    extract_and_write(input_file_names[i], output_file_names[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "Jlw5uDr1TZrJ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract 50 from testing dataset:\n",
        "\n",
        "import json\n",
        "\n",
        "def extract_and_write(input_file, output_file):\n",
        "    with open(input_file, 'r') as input_file:\n",
        "        text = input_file.read()\n",
        "\n",
        "    # Remove commas\n",
        "    text = text.replace(',', '')\n",
        "\n",
        "    lines = text.strip().split('\\n')\n",
        "    first_5_entries = lines[:50]\n",
        "\n",
        "    with open(output_file, 'w') as output_file:\n",
        "        json.dump(first_5_entries, output_file, indent=2)\n",
        "\n",
        "# Input file names\n",
        "input_file_names = [\n",
        "    '/content/drive/MyDrive/testAuthor.txt',\n",
        "    '/content/drive/MyDrive/testFos.txt',\n",
        "    '/content/drive/MyDrive/testRating.txt'\n",
        "]\n",
        "\n",
        "# Output file names\n",
        "output_file_names = [\n",
        "    '/content/drive/MyDrive/final50_testAuthor.json',\n",
        "    '/content/drive/MyDrive/final50_testFos.json',\n",
        "    '/content/drive/MyDrive/final50_testRating.json'\n",
        "]\n",
        "\n",
        "# Extract and write for each input-output pair\n",
        "for i in range(3):\n",
        "    extract_and_write(input_file_names[i], output_file_names[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "IsbcvGftpklX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --------------- convert json to pickle (training and testing data) -----------------\n",
        "\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# Specify the path for the input JSON file\n",
        "input_file_path1 = '/content/drive/MyDrive/unique_trainAuthor.json'\n",
        "input_file_path2 = '/content/drive/MyDrive/unique_trainFos.json'\n",
        "input_file_path3 = '/content/drive/MyDrive/unique_ratings_train.json'\n",
        "input_file_path4 = '/content/drive/MyDrive/unique_testAuthor.json'\n",
        "input_file_path5 = '/content/drive/MyDrive/unique_testFos.json'\n",
        "input_file_path6 = '/content/drive/MyDrive/unique_ratings_test.json'\n",
        "\n",
        "\n",
        "# Specify the path for the output pickle file\n",
        "pickle_file_path1 = '/content/drive/MyDrive/unique_trainAuthor.pkl'\n",
        "pickle_file_path2 = '/content/drive/MyDrive/unique_trainFos.pkl'\n",
        "pickle_file_path3 = '/content/drive/MyDrive/unique_ratings_train.pkl'\n",
        "pickle_file_path4 = '/content/drive/MyDrive/unique_testAuthor.pkl'\n",
        "pickle_file_path5 = '/content/drive/MyDrive/unique_testFos.pkl'\n",
        "pickle_file_path6 = '/content/drive/MyDrive/unique_ratings_test.pkl'\n",
        "\n",
        "\n",
        "# Read data from the JSON file\n",
        "with open(input_file_path1, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path2, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path3, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path4, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path5, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "with open(input_file_path6, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "\n",
        "\n",
        "# Write data to the pickle file\n",
        "with open(pickle_file_path1, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path2, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path3, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path4, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path5, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n",
        "\n",
        "with open(pickle_file_path6, 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n"
      ],
      "metadata": {
        "id": "s0oS7HT2OTEj"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#get Authors and Fos list ---------------\n",
        "\n",
        "import json\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_authors_to_fos.json'  # Replace with the path to your input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Create lists for fos and authors\n",
        "fos_list = []\n",
        "authors_list = []\n",
        "\n",
        "# Iterate through the input data\n",
        "for author, fos in input_data.items():\n",
        "    fos_list.extend(fos)\n",
        "    authors_list.extend([int(author)] * len(fos))\n",
        "\n",
        "# Save the fos list to a JSON file\n",
        "fos_list_file_path = '/content/drive/MyDrive/unique_fos_list.json'  # Replace with your desired output file path\n",
        "with open(fos_list_file_path, 'w') as fos_list_file:\n",
        "    json.dump(fos_list, fos_list_file, indent=2)\n",
        "\n",
        "# Save the authors list to a JSON file\n",
        "authors_list_file_path = '/content/drive/MyDrive/unique_authors_list.json'  # Replace with your desired output file path\n",
        "with open(authors_list_file_path, 'w') as authors_list_file:\n",
        "    json.dump(authors_list, authors_list_file, indent=2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tlsaJgNr9QFt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Input file path\n",
        "input_file_path = '/content/drive/MyDrive/weights.json'  # Replace with the path to your input JSON file\n",
        "\n",
        "# Load the input JSON file\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_data = json.load(input_file)\n",
        "\n",
        "# Extract ratings from input data\n",
        "ratings = [rating for concepts in input_data.values() for rating in concepts.values()]\n",
        "\n",
        "# Save the output data to a JSON file\n",
        "output_file_path = '/content/drive/MyDrive/unique_ratings.json'  # Replace with your desired output file path\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(ratings, output_file, indent=2)\n",
        "\n",
        "print(\"JSON file created successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIFBk8AYXOML",
        "outputId": "79af154d-e816-49d0-bffd-58d42b148f62"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON file created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the Authors (values) data into training and testing\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/unique_authors_list.json'\n",
        "\n",
        "# Specify the paths for the output training and testing JSON files\n",
        "output_train_file_path = '/content/drive/MyDrive/unique_trainAuthor.json'\n",
        "output_test_file_path = '/content/drive/MyDrive/unique_testAuthor.json'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Remove trailing commas and convert lines to integers\n",
        "def convert_to_int(line):\n",
        "    try:\n",
        "        return int(line.strip().rstrip(','))\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets, handling non-integer values\n",
        "train_lines = [convert_to_int(line) for line in lines[:split_index] if convert_to_int(line) is not None]\n",
        "test_lines = [convert_to_int(line) for line in lines[split_index:] if convert_to_int(line) is not None]\n",
        "\n",
        "# Write training data to the output JSON file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    json.dump(train_lines, train_file, indent=2)\n",
        "\n",
        "# Write testing data to the output JSON file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    json.dump(test_lines, test_file, indent=2)\n"
      ],
      "metadata": {
        "id": "05xJ5SHm_b9B"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the values (Fos) data into training and testing\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path for the input text file\n",
        "input_file_path = '/content/drive/MyDrive/unique_fos_list.json'\n",
        "\n",
        "# Specify the paths for the output training and testing JSON files\n",
        "output_train_file_path = '/content/drive/MyDrive/unique_trainFos.json'\n",
        "output_test_file_path = '/content/drive/MyDrive/unique_testFos.json'\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Read lines from the input file\n",
        "with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
        "    lines = input_file.readlines()\n",
        "\n",
        "# Remove trailing commas and convert lines to integers\n",
        "def convert_to_int(line):\n",
        "    try:\n",
        "        return int(line.strip().rstrip(','))\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(lines) * split_ratio)\n",
        "\n",
        "# Split the lines into training and testing sets, handling non-integer values\n",
        "train_lines = [convert_to_int(line) for line in lines[:split_index] if convert_to_int(line) is not None]\n",
        "test_lines = [convert_to_int(line) for line in lines[split_index:] if convert_to_int(line) is not None]\n",
        "\n",
        "# Write training data to the output JSON file\n",
        "with open(output_train_file_path, 'w', encoding='utf-8') as train_file:\n",
        "    json.dump(train_lines, train_file, indent=2)\n",
        "\n",
        "# Write testing data to the output JSON file\n",
        "with open(output_test_file_path, 'w', encoding='utf-8') as test_file:\n",
        "    json.dump(test_lines, test_file, indent=2)\n"
      ],
      "metadata": {
        "id": "o99-AGtG_fdu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the ratings data into training and testing\n",
        "\n",
        "import json\n",
        "\n",
        "# Load the input JSON file\n",
        "input_file_path = '/content/drive/MyDrive/unique_ratings.json'  # Replace with your actual file path\n",
        "with open(input_file_path, 'r') as input_file:\n",
        "    input_list = json.load(input_file)\n",
        "\n",
        "# Set the ratio for splitting the data (e.g., 80% training, 20% testing)\n",
        "split_ratio = 0.8\n",
        "\n",
        "# Calculate the index for splitting\n",
        "split_index = int(len(input_list) * split_ratio)\n",
        "\n",
        "# Split the list into training and testing sets\n",
        "train_data = input_list[:split_index]\n",
        "test_data = input_list[split_index:]\n",
        "\n",
        "# Specify the paths for the output training and testing files\n",
        "output_train_file_path = '/content/drive/MyDrive/unique_ratings_train.json'\n",
        "output_test_file_path = '/content/drive/MyDrive/unique_ratings_test.json'\n",
        "\n",
        "# Write training data to the output file\n",
        "with open(output_train_file_path, 'w') as train_file:\n",
        "    json.dump(train_data, train_file, indent=2)\n",
        "\n",
        "# Write testing data to the output file\n",
        "with open(output_test_file_path, 'w') as test_file:\n",
        "    json.dump(test_data, test_file, indent=2)\n"
      ],
      "metadata": {
        "id": "_jk9xR_z_jZi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------- COMBINED FILE ---------------------------------\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the file paths for the JSON and TXT files\n",
        "\n",
        "json_file_paths = ['/content/drive/MyDrive/unique_authors_to_fos.json', '/content/drive/MyDrive/uniqueId_opinion.json',\n",
        "              '/content/drive/MyDrive/unique_fos_to_authors.json', '/content/drive/MyDrive/uniqueId_opinionItem.json']\n",
        "# txt_file_paths = ['/content/drive/MyDrive/trainAuthor.txt', '/content/drive/MyDrive/trainFos.txt', '/content/drive/MyDrive/trainRating.txt',\n",
        "#                   '/content/drive/MyDrive/testAuthor.txt', '/content/drive/MyDrive/testFos.txt', '/content/drive/MyDrive/testRating.txt']\n",
        "\n",
        "\n",
        "# Specify the output file path for the combined data\n",
        "output_file_path = '/content/drive/MyDrive/unique_combinedFile.json'\n",
        "\n",
        "# Initialize an empty list to store individual file data\n",
        "all_data = []\n",
        "\n",
        "# Read data from JSON files\n",
        "for json_path in json_file_paths:\n",
        "    with open(json_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "    all_data.append(data)\n",
        "\n",
        "# Read data from TXT files\n",
        "# for txt_path in txt_file_paths:\n",
        "#     with open(txt_path, 'r') as txt_file:\n",
        "#         data = {'text_data': txt_file.read()}\n",
        "#     all_data.append(data)\n",
        "\n",
        "\n",
        "\n",
        "# Write the combined data to the output file\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(all_data, output_file, indent=2)\n"
      ],
      "metadata": {
        "id": "9wwdLcF0eV3F"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------- MERGED/COMBINED FILES INTO ONE ----------------\n",
        "\n",
        "import json\n",
        "\n",
        "def load_json_or_text(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        try:\n",
        "            # Attempt to load as JSON\n",
        "            return json.load(file)\n",
        "        except json.JSONDecodeError:\n",
        "            # If it's not JSON, treat it as a text file\n",
        "            lines = file.readlines()\n",
        "            return {str(i): tuple(map(float, line.strip().split(','))) for i, line in enumerate(lines)}\n",
        "\n",
        "# Specify the paths to your files\n",
        "file1_path = '/content/drive/MyDrive/unique_combinedFile.json'\n",
        "file2_path = '/content/drive/MyDrive/unique_trainAuthor.json'\n",
        "file3_path = '/content/drive/MyDrive/unique_trainFos.json'\n",
        "file4_path = '/content/drive/MyDrive/unique_ratings_train.json'\n",
        "file5_path = '/content/drive/MyDrive/unique_testAuthor.json'\n",
        "file6_path = '/content/drive/MyDrive/unique_testFos.json'\n",
        "file7_path = '/content/drive/MyDrive/unique_ratings_test.json'\n",
        "file8_path = '/content/drive/MyDrive/unique_author_to_coauthors.json'\n",
        "file9_path = '/content/drive/MyDrive/embedding.txt'\n",
        "\n",
        "# Load data from each file\n",
        "file1_data = load_json_or_text(file1_path)\n",
        "file2_data = load_json_or_text(file2_path)\n",
        "file3_data = load_json_or_text(file3_path)\n",
        "file4_data = load_json_or_text(file4_path)\n",
        "file5_data = load_json_or_text(file5_path)\n",
        "file6_data = load_json_or_text(file6_path)\n",
        "file7_data = load_json_or_text(file7_path)\n",
        "file8_data = load_json_or_text(file8_path)\n",
        "file9_data = load_json_or_text(file9_path)\n",
        "\n",
        "# Combine all data into a single list\n",
        "combined_data = [\n",
        "    file1_data[0],\n",
        "    file1_data[1],\n",
        "    file1_data[2],\n",
        "    file1_data[3],\n",
        "    file2_data,\n",
        "    file3_data,\n",
        "    file4_data,\n",
        "    file5_data,\n",
        "    file6_data,\n",
        "    file7_data,\n",
        "    file8_data,\n",
        "    file9_data\n",
        "]\n",
        "\n",
        "# Convert the combined data to JSON format\n",
        "json_data = json.dumps(combined_data, indent=2)\n",
        "\n",
        "# Save JSON data to a file\n",
        "with open('/content/drive/MyDrive/mergedFile.json', 'w') as json_file:\n",
        "    json_file.write(json_data)\n"
      ],
      "metadata": {
        "id": "fSisqyxKx4Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "\n",
        "# Load the JSON data\n",
        "with open('/content/drive/MyDrive/Final_combinedFile.json', 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Save the data as a pickle file\n",
        "with open('/content/drive/MyDrive/Final_combinedFile.pickle', 'wb') as pickle_file:\n",
        "    pickle.dump(data, pickle_file)\n"
      ],
      "metadata": {
        "id": "YW9WV4bUeSTw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "\n",
        "# Load JSON file\n",
        "with open('your_input_file.json', 'r') as json_file:\n",
        "    json_data = json.load(json_file)\n",
        "\n",
        "# Save as pickle file\n",
        "with open('your_output_file.pickle', 'wb') as pickle_file:\n",
        "    pickle.dump(json_data, pickle_file)\n"
      ],
      "metadata": {
        "id": "5FxMUVHpxMg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the JSON file\n",
        "with open('/content/drive/MyDrive/Final_combinedFile.json', 'r') as file:\n",
        "    json_structure = json.load(file)\n",
        "\n",
        "# Check if the last element is a list\n",
        "if isinstance(json_structure[-1], list):\n",
        "    # Remove the last entry from the last list\n",
        "    removed_entry = json_structure[-1].pop()\n",
        "\n",
        "    # Check if the third element is a list\n",
        "    if isinstance(json_structure[2], list):\n",
        "        # Add the removed entry to the last position of the third list\n",
        "        json_structure[2].append(removed_entry)\n",
        "\n",
        "        # Write the modified JSON structure to a new file\n",
        "        with open('/content/drive/MyDrive/FinalFilePleaseGod_combinedFile.json', 'w') as output_file:\n",
        "            json.dump(json_structure, output_file, indent=2)\n",
        "\n",
        "        print(f\"Removed entry: {removed_entry}\")\n",
        "        print(\"Output JSON file created successfully.\")\n",
        "    else:\n",
        "        print(\"The third element in the JSON structure is not a list.\")\n",
        "else:\n",
        "    print(\"The last element in the JSON structure is not a list.\")"
      ],
      "metadata": {
        "id": "Dt2bXVr3Nkyd",
        "outputId": "9003244f-8788-4d57-b107-05fcf00be3c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The last element in the JSON structure is not a list.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the JSON file\n",
        "with open('/content/gdrive/MyDrive/Final_combinedFile.json', 'r') as file:\n",
        "    input_json = json.load(file)\n",
        "\n",
        "# Extracting data\n",
        "history_u_lists = {str(k): v for d in input_json[3:50860603] for k, v in d.items()}\n",
        "history_ur_lists = {str(k): v for d in input_json[50860603:101721206] for k, v in d.items()}\n",
        "history_v_lists = {str(i): entry for i, entry in enumerate(input_json[101721206:152581809])}\n",
        "history_vr_lists = {str(i): entry for i, entry in enumerate(input_json[152581809:203442412])}\n",
        "train_u = input_json[203442412]\n",
        "train_v = input_json[203442413]\n",
        "train_r = input_json[203442414]\n",
        "test_u = input_json[203442415]\n",
        "test_v = input_json[203442416]\n",
        "test_r = input_json[203442417]\n",
        "social_adj_lists = input_json[203442418]\n",
        "ratings_list = input_json[203442419]\n",
        "\n",
        "# Organize the data into a dictionary\n",
        "output_data = {\n",
        "    \"history_u_lists\": history_u_lists,\n",
        "    \"history_ur_lists\": history_ur_lists,\n",
        "    \"history_v_lists\": history_v_lists,\n",
        "    \"history_vr_lists\": history_vr_lists,\n",
        "    \"train_u\": train_u,\n",
        "    \"train_v\": train_v,\n",
        "    \"train_r\": train_r,\n",
        "    \"test_u\": test_u,\n",
        "    \"test_v\": test_v,\n",
        "    \"test_r\": test_r,\n",
        "    \"social_adj_lists\": social_adj_lists,\n",
        "    \"ratings_list\": ratings_list\n",
        "}\n",
        "\n",
        "# Write the output to a new JSON file\n",
        "with open('/content/gdrive/MyDrive/FinalOutput_combinedFile.json', 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=4)\n"
      ],
      "metadata": {
        "id": "Rk0Y2bORJHTp",
        "outputId": "9e9174a5-02df-4491-bf3b-048602c69767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0f5ecd6483db>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Extracting data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory_u_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50860603\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mhistory_ur_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50860603\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m101721206\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhistory_v_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m101721206\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m152581809\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-0f5ecd6483db>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Extracting data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mhistory_u_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50860603\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mhistory_ur_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50860603\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m101721206\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhistory_v_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m101721206\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m152581809\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the JSON file\n",
        "with open('your_input_file.json', 'r') as file:\n",
        "    input_json = json.load(file)\n",
        "\n",
        "# Extracting data\n",
        "history_u_lists = {str(k): v for d in input_json[:2] for k, v in d.items()}\n",
        "history_ur_lists = {str(k): v for d in input_json[2:4] for k, v in d.items()}\n",
        "history_v_lists = {str(i): entry for i, entry in enumerate(input_json[4:6])}\n",
        "history_vr_lists = {str(i): entry for i, entry in enumerate(input_json[6:8])}\n",
        "train_u = input_json[8]\n",
        "train_v = input_json[9]\n",
        "train_r = input_json[10]\n",
        "test_u = input_json[11]\n",
        "test_v = input_json[12]\n",
        "test_r = input_json[13]\n",
        "social_adj_lists = input_json[14]\n",
        "ratings_list = input_json[15]\n",
        "\n",
        "# Organize the data into a dictionary\n",
        "output_data = {\n",
        "    \"history_u_lists\": history_u_lists,\n",
        "    \"history_ur_lists\": history_ur_lists,\n",
        "    \"history_v_lists\": history_v_lists,\n",
        "    \"history_vr_lists\": history_vr_lists,\n",
        "    \"train_u\": train_u,\n",
        "    \"train_v\": train_v,\n",
        "    \"train_r\": train_r,\n",
        "    \"test_u\": test_u,\n",
        "    \"test_v\": test_v,\n",
        "    \"test_r\": test_r,\n",
        "    \"social_adj_lists\": social_adj_lists,\n",
        "    \"ratings_list\": ratings_list\n",
        "}\n",
        "\n",
        "# Write the output to a new JSON file\n",
        "with open('output_file.json', 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=4)\n"
      ],
      "metadata": {
        "id": "5ziIYLEijaXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Read the JSON file\n",
        "with open('/content/gdrive/MyDrive/Final_combinedFile.json', 'r') as file:\n",
        "    input_json = json.load(file)\n",
        "\n",
        "# Extracting data\n",
        "history_u_lists = {str(k): v for k, v in input_json[0].items()}\n",
        "history_ur_lists = {str(k): v for k, v in input_json[1].items()}\n",
        "history_v_lists = {str(k): v for k, v in enumerate(input_json[2])}\n",
        "history_vr_lists = {str(k): v for k, v in enumerate(input_json[3])}\n",
        "train_u = input_json[4]\n",
        "train_v = input_json[5]\n",
        "train_r = input_json[6]\n",
        "test_u = input_json[7]\n",
        "test_v = input_json[8]\n",
        "test_r = input_json[9]\n",
        "social_adj_lists = input_json[10]\n",
        "ratings_list = input_json[11]\n",
        "\n",
        "# Organize the data into a dictionary\n",
        "output_data = {\n",
        "    \"history_u_lists\": history_u_lists,\n",
        "    \"history_ur_lists\": history_ur_lists,\n",
        "    \"history_v_lists\": history_v_lists,\n",
        "    \"history_vr_lists\": history_vr_lists,\n",
        "    \"train_u\": train_u,\n",
        "    \"train_v\": train_v,\n",
        "    \"train_r\": train_r,\n",
        "    \"test_u\": test_u,\n",
        "    \"test_v\": test_v,\n",
        "    \"test_r\": test_r,\n",
        "    \"social_adj_lists\": social_adj_lists,\n",
        "    \"ratings_list\": ratings_list\n",
        "}\n",
        "\n",
        "# Write the output to a new JSON file\n",
        "with open('/content/gdrive/MyDrive/FinalOutput_combinedFile.json', 'w') as output_file:\n",
        "    json.dump(output_data, output_file, indent=4)\n"
      ],
      "metadata": {
        "id": "wG5Lujaqu0Nj",
        "outputId": "f92d225a-3bdb-41d6-f1c0-ab73ccd404e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-643b03c98235>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Write the output to a new JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/MyDrive/FinalOutput_combinedFile.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter notebook --NotebookApp.iopub_data_rate_limit=1000000000\n"
      ],
      "metadata": {
        "id": "9ZynNktROt7V",
        "outputId": "03b2ad80-c0ec-408a-ffe5-8742a17f76b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/usr/local/etc/jupyter/jupyter_notebook_config.d/panel-client-jupyter.json\n",
            "    \t/usr/local/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/usr/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/root/.local/etc/jupyter/jupyter_notebook_config.json\n",
            "|DEBUG|Paths used for configuration of jupyter_notebook_config: \n",
            "    \t/root/.jupyter/jupyter_notebook_config.json\n",
            "\n",
            "  _   _          _      _\n",
            " | | | |_ __  __| |__ _| |_ ___\n",
            " | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "  \\___/| .__/\\__,_\\__,_|\\__\\___|\n",
            "       |_|\n",
            "                       \n",
            "Read the migration plan to Notebook 7 to learn about the new features and the actions to take if you are using extensions.\n",
            "\n",
            "https://jupyter-notebook.readthedocs.io/en/latest/migrate_to_notebook7.html\n",
            "\n",
            "Please note that updating to Notebook 7 might break some of your extensions.\n",
            "\n",
            "|INFO|google.colab serverextension initialized.\n",
            "|INFO|Serving notebooks from local directory: /content\n",
            "|INFO|Jupyter Notebook 6.5.5 is running at:\n",
            "|INFO|http://localhost:8888/?token=3f8edcb5dd0a0d8e4412d2accc03d422f1e0386971f14d46\n",
            "|INFO| or http://127.0.0.1:8888/?token=3f8edcb5dd0a0d8e4412d2accc03d422f1e0386971f14d46\n",
            "|INFO|Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n",
            "|CRITICAL|\n",
            "    \n",
            "    To access the notebook, open this file in a browser:\n",
            "        file:///root/.local/share/jupyter/runtime/nbserver-8903-open.html\n",
            "    Or copy and paste one of these URLs:\n",
            "        http://localhost:8888/?token=3f8edcb5dd0a0d8e4412d2accc03d422f1e0386971f14d46\n",
            "     or http://127.0.0.1:8888/?token=3f8edcb5dd0a0d8e4412d2accc03d422f1e0386971f14d46\n",
            "|INFO|interrupted\n",
            "|CRITICAL|Shutting down...\n",
            "|INFO|interrupted\n",
            "|CRITICAL|Shutting down...\n",
            "|INFO|Shutting down 0 kernels\n",
            "|INFO|Shutting down 0 terminals\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}