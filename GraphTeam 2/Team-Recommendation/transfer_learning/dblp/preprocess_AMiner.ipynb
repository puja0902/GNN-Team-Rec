{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/pujasharma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pujasharma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import networkx as nx\n",
    "# import utils.preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from scipy.sparse import coo_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (1.4.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from scikit-learn) (3.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /Users/pujasharma/miniconda3/envs/tensorflow/lib/python3.10/site-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4g/l0rw4rcx2y56vrgxqnjh2kqm0000gn/T/ipykernel_5450/696679803.py:1: DeprecationWarning: Please import `coo_matrix` from the `scipy.sparse` namespace; the `scipy.sparse.coo` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
      "  dataset = pickle.load(open('raw/AMiner/dblp_preprocessed_dataset.pkl', 'rb'))\n"
     ]
    }
   ],
   "source": [
    "dataset = pickle.load(open('raw/AMiner/dblp_preprocessed_dataset.pkl', 'rb'))\n",
    "train_test_idx = pickle.load(open('raw/AMiner/Train_Test_indices.pkl', 'rb'))\n",
    "# dataset = pickle.load(open('raw/AMiner/dblp_preprocessed_dataset_V2.2.pkl', 'rb'))\n",
    "# train_test_idx = pickle.load(open('raw/AMiner/Train_Test_indices_V2.2.pkl', 'rb'))\n",
    "docID_venue = pickle.load(open('raw/AMiner/documentID_venue.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_label = pd.DataFrame(columns=['author_id', 'label', 'author_name'])\n",
    "paper_author = pd.DataFrame(columns=['paper_id', 'author_id'])\n",
    "paper_conf = pd.DataFrame(columns=['paper_id', 'conf_id'])\n",
    "paper_term = pd.DataFrame(columns=['paper_id', 'term_id'])\n",
    "papers = pd.DataFrame(columns=['paper_id', 'paper_title'])\n",
    "terms = pd.DataFrame(columns=['term_id', 'term'])\n",
    "confs = pd.DataFrame(columns=['conf_id', 'conf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning venue names\n",
    "publication_list = ['sigmod', 'vldb', 'icde', 'icdt', 'edbt', 'pods', 'kdd', 'www',\n",
    "                      'sdm', 'pkdd', 'icdm', 'cikm', 'aaai', 'icml', 'ecml', 'colt',\n",
    "                      'uai', 'soda', 'focs', 'stoc', 'stacs']\n",
    "\n",
    "for i, record in enumerate(docID_venue):\n",
    "    venue = record[1]\n",
    "    for pub in publication_list:\n",
    "        if pub in venue.lower():\n",
    "            docID_venue[i][1] = pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_counter = Counter()\n",
    "terms_counter = Counter()\n",
    "for record in dataset:\n",
    "    paper_id = record[0]\n",
    "    skillIdx = record[1].todense().nonzero()[1]\n",
    "    terms_counter.update(skillIdx)\n",
    "    authorIdx = record[2].todense().nonzero()[1]\n",
    "    authors_counter.update(authorIdx)\n",
    "    \n",
    "    for authorId in authorIdx:\n",
    "        paper_author = paper_author._append({'paper_id': paper_id, 'author_id': authorId}, ignore_index=True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    for skillId in skillIdx:\n",
    "        paper_term = paper_term._append({'paper_id': paper_id, 'term_id': skillId}, ignore_index=True)\n",
    "        \n",
    "    papers = papers._append({'paper_id': paper_id, 'paper_title': 'na'}, ignore_index=True)\n",
    "\n",
    "unique_authors_idx = list(authors_counter.keys())\n",
    "for unique_authors_id in unique_authors_idx:\n",
    "    author_label = author_label._append({'author_id': unique_authors_id, 'label': -1, 'author_name': 'na'}, ignore_index=True)\n",
    "    \n",
    "unique_terms_idx = list(terms_counter.keys())\n",
    "for unique_terms_id in unique_terms_idx:\n",
    "    terms = terms._append({'term_id': unique_terms_id, 'term': 'na'}, ignore_index=True)\n",
    "    \n",
    "conf_counter = Counter()\n",
    "for record in docID_venue:\n",
    "    paper_id = record[0]\n",
    "    conf_counter.update([record[1]])\n",
    "venues = list(conf_counter.keys())\n",
    "\n",
    "conf_confID = {}\n",
    "for i, venue in enumerate(venues):\n",
    "    confs = confs._append({'conf_id': i, 'conf': venue}, ignore_index=True)\n",
    "    conf_confID.update({venue: i})\n",
    "    \n",
    "for record in docID_venue:\n",
    "    paper_id = record[0]\n",
    "    conf_id = conf_confID[record[1]]\n",
    "    paper_conf = paper_conf._append({'paper_id': paper_id, 'conf_id': conf_id}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_counter = Counter()\n",
    "terms_counter = Counter()\n",
    "author_size_counter = Counter()\n",
    "for record in dataset:\n",
    "    paper_id = record[0]\n",
    "    skillIdx = record[1].todense().nonzero()[1]\n",
    "    terms_counter.update(skillIdx)\n",
    "    authorIdx = record[2].todense().nonzero()[1]\n",
    "    authors_counter.update(authorIdx)\n",
    "    author_size_counter.update([record[2].todense().shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2470\n",
      "Counter({2470: 33002})\n"
     ]
    }
   ],
   "source": [
    "print(len(unique_authors_idx))\n",
    "print(author_size_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# author_label = pd.read_csv('raw/AMiner/author_label.txt', sep='\\t', header=None, names=['author_id', 'label', 'author_name'], keep_default_na=False, encoding='utf-8')\n",
    "# paper_author = pd.read_csv('raw/AMiner/paper_author.txt', sep='\\t', header=None, names=['paper_id', 'author_id'], keep_default_na=False, encoding='utf-8')\n",
    "# paper_conf = pd.read_csv('raw/AMiner/paper_conf.txt', sep='\\t', header=None, names=['paper_id', 'conf_id'], keep_default_na=False, encoding='utf-8')\n",
    "# paper_term = pd.read_csv('raw/AMiner/paper_term.txt', sep='\\t', header=None, names=['paper_id', 'term_id'], keep_default_na=False, encoding='utf-8')\n",
    "# papers = pd.read_csv('raw/AMiner/paper.txt', sep='\\t', header=None, names=['paper_id', 'paper_title'], keep_default_na=False, encoding='cp1252')\n",
    "# terms = pd.read_csv('raw/AMiner/term.txt', sep='\\t', header=None, names=['term_id', 'term'], keep_default_na=False, encoding='utf-8')\n",
    "# confs = pd.read_csv('raw/AMiner/conf.txt', sep='\\t', header=None, names=['conf_id', 'conf'], keep_default_na=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers : 33002\n",
      "Number of papers : 33002\n",
      "Number of conferences  21\n",
      "Number of authors  2470\n",
      "Number of terms  2000\n",
      "Number of papers  33002\n",
      " Total entities ::  37493\n"
     ]
    }
   ],
   "source": [
    "authors = author_label['author_id'].to_list()\n",
    "paper_author = paper_author[paper_author['author_id'].isin(authors)].reset_index(drop=True)\n",
    "valid_papers = paper_author['paper_id'].unique()\n",
    "print('Number of papers :', len(valid_papers))\n",
    "\n",
    "papers = papers[papers['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "paper_conf = paper_conf[paper_conf['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "print('Number of papers :', len(paper_conf))\n",
    "\n",
    "paper_term = paper_term[paper_term['paper_id'].isin(valid_papers)].reset_index(drop=True)\n",
    "valid_terms = paper_term['term_id'].unique()\n",
    "terms = terms[terms['term_id'].isin(valid_terms)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "author_label = author_label.sort_values('author_id').reset_index(drop=True)\n",
    "papers = papers.sort_values('paper_id').reset_index(drop=True)\n",
    "terms = terms.sort_values('term_id').reset_index(drop=True)\n",
    "confs = confs.sort_values('conf_id').reset_index(drop=True)\n",
    "\n",
    "print('Number of conferences ', len(confs))\n",
    "print('Number of authors ', len(author_label))\n",
    "print('Number of terms ', len(terms))\n",
    "print('Number of papers ', len(papers))\n",
    "\n",
    "authors_list = list(author_label['author_id'])\n",
    "papers_list = list(papers['paper_id'])\n",
    "term_list = list(terms['term_id'])\n",
    "conf_list = list(confs['conf_id'])\n",
    "dim = len(authors_list) + len(papers_list) + len(term_list) + len(confs)\n",
    "print(' Total entities :: ', dim)\n",
    "\n",
    "\n",
    "author_id_mapping = {row['author_id']: i for i, row in author_label.iterrows()}\n",
    "paper_id_mapping = {row['paper_id']: i + len(author_label) for i, row in papers.iterrows()}\n",
    "term_id_mapping = {row['term_id']: i + len(author_label) + len(papers) for i, row in terms.iterrows()}\n",
    "conf_id_mapping = {row['conf_id']: i + len(author_label) + len(papers) + len(terms) for i, row in confs.iterrows()}\n",
    "\n",
    "\n",
    "entity_id_map = pd.DataFrame(\n",
    "    columns=['domain', 'entity_id','serial_id']\n",
    ")\n",
    "type_dict = { 'author': author_id_mapping, 'paper': paper_id_mapping, 'term': term_id_mapping, 'conf': conf_id_mapping }\n",
    "for _type,_dict in type_dict.items():\n",
    "    i = list(_dict.keys())\n",
    "    j = list(_dict.values())\n",
    "    _df = pd.DataFrame( data = {'entity_id': i ,'serial_id': j } )\n",
    "    _df['domain'] = _type\n",
    "    entity_id_map = entity_id_map._append(_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "# ======================================================\n",
    "# Save data\n",
    "# ======================================================\n",
    "data_save_path = 'processed_data/'\n",
    "if not os.path.exists('processed_data'):\n",
    "    os.mkdir('processed_data')\n",
    "if not os.path.exists(data_save_path):\n",
    "    os.mkdir(data_save_path)\n",
    "entity_id_map.to_csv( os.path.join( data_save_path, 'entity_id_mapping.csv') ) \n",
    "\n",
    "# Create graph data\n",
    "nodes_author_df = pd.DataFrame( data = { 'author' : list(author_id_mapping.values()) })\n",
    "nodes_paper_df = pd.DataFrame(  data = { 'paper' : list(paper_id_mapping.values()) } )\n",
    "nodes_term_df = pd.DataFrame( data = { 'term' : list(term_id_mapping.values()) } )\n",
    "nodes_conf_df = pd.DataFrame(  data = { 'conf' : list(conf_id_mapping.values()) } )\n",
    "\n",
    "nodes_author_df.to_csv(os.path.join(data_save_path,'nodes_author.csv'),index = False)\n",
    "nodes_paper_df.to_csv(os.path.join(data_save_path,'nodes_paper.csv'),index = False)\n",
    "nodes_term_df.to_csv(os.path.join(data_save_path,'nodes_term.csv'),index = False)\n",
    "nodes_conf_df.to_csv(os.path.join(data_save_path,'nodes_conf.csv'),index = False)\n",
    "\n",
    "PA_edge_list = []\n",
    "for _, row in paper_author.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = author_id_mapping[row['author_id']]\n",
    "    PA_edge_list.append((idx1,idx2))\n",
    "    \n",
    "df = pd.DataFrame ( data =  np.array(PA_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PA_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "PT_edge_list = []\n",
    "for _, row in paper_term.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = term_id_mapping[row['term_id']]\n",
    "    PT_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data =  np.array(PT_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PT_edges.csv')\n",
    "df.to_csv(fpath, index=False)\n",
    "    \n",
    "\n",
    "PC_edge_list = []\n",
    "for _, row in paper_conf.iterrows():\n",
    "    idx1 = paper_id_mapping[row['paper_id']]\n",
    "    idx2 = conf_id_mapping[row['conf_id']]\n",
    "    PC_edge_list.append((idx1,idx2))\n",
    "\n",
    "df = pd.DataFrame ( data = np.array(PC_edge_list), columns = ['source','target'])\n",
    "fpath = os.path.join(data_save_path, 'PC_edges.csv')\n",
    "df.to_csv(fpath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6085085752378643\n",
      "6.523331919277619\n",
      "1571.5238095238096\n"
     ]
    }
   ],
   "source": [
    "ap_counter = Counter()\n",
    "tp_counter = Counter()\n",
    "pc_counter = Counter()\n",
    "\n",
    "for i, _ in PA_edge_list:\n",
    "    ap_counter.update([i])\n",
    "print(np.mean(list(ap_counter.values())))\n",
    "\n",
    "for i, _ in PT_edge_list:\n",
    "    tp_counter.update([i])\n",
    "print(np.mean(list(tp_counter.values())))\n",
    "\n",
    "for _, i in PC_edge_list:\n",
    "    pc_counter.update([i])\n",
    "print(np.mean(list(pc_counter.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Create data for HIN2Vec\n",
    "# ==============================\n",
    "\n",
    "df = pd.DataFrame(columns=['node1', 'node2','rel'])\n",
    "for edge in PA_edge_list:\n",
    "    df = df._append({'node1':edge[0],'node2':edge[1],'rel': 0},ignore_index=True )\n",
    "\n",
    "for edge in PT_edge_list:\n",
    "    df = df._append({'node1':edge[0],'node2':edge[1],'rel': 1},ignore_index=True )\n",
    "    \n",
    "for edge in PC_edge_list:\n",
    "    df = df._append({'node1':edge[0],'node2':edge[1],'rel': 2},ignore_index=True )\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['node1'] = df['node1'].astype(int)\n",
    "df['node2'] = df['node2'].astype(int)\n",
    "df['rel'] = df['rel'].astype(int)\n",
    "fpath = os.path.join(data_save_path,'hin2vec_dblp_input.txt')\n",
    "df.to_csv( fpath, index = None, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>node1</th>\n",
       "      <th>node2</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2471</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2472</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2473</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2473</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301364</th>\n",
       "      <td>35467</td>\n",
       "      <td>37472</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301365</th>\n",
       "      <td>35468</td>\n",
       "      <td>37472</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301366</th>\n",
       "      <td>35469</td>\n",
       "      <td>37478</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301367</th>\n",
       "      <td>35470</td>\n",
       "      <td>37478</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301368</th>\n",
       "      <td>35471</td>\n",
       "      <td>37478</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>301369 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        node1  node2  rel\n",
       "0        2470      0    0\n",
       "1        2471      1    0\n",
       "2        2472      2    0\n",
       "3        2473      3    0\n",
       "4        2473      4    0\n",
       "...       ...    ...  ...\n",
       "301364  35467  37472    2\n",
       "301365  35468  37472    2\n",
       "301366  35469  37478    2\n",
       "301367  35470  37478    2\n",
       "301368  35471  37478    2\n",
       "\n",
       "[301369 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>serial_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>author</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>author</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>author</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>author</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>author</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   domain entity_id serial_id\n",
       "0  author         0         0\n",
       "1  author         1         1\n",
       "2  author         2         2\n",
       "3  author         3         3\n",
       "4  author         4         4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_map.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "domain         paper\n",
       "entity_id    1019536\n",
       "serial_id      14000\n",
       "Name: 14000, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_id_map.loc[14000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
